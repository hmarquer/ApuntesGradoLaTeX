\section{Sistemas lineales de primer orden}

Sea $\forall t \in (a, b) : X'(t) = A(t)X(t) + g(t)$, con $a_j^i, g_j^i \in C((a,b))$ y $d\in \N \we d > 1$.
\[A(t) = \begin{pmatrix}
		a_{11}(t) & \cdots & a_{1d}(t) \\
		\vdots    & \ddots & \vdots    \\
		a_{d1}(t) & \cdots & a_{dd}(t)
	\end{pmatrix}, \quad X(t) = \begin{pmatrix}
		X^1(t) \\
		\vdots \\
		X^d(t)
	\end{pmatrix}, \quad g(t) = \begin{pmatrix}
		g^1(t) \\
		\vdots \\
		g^d(t)
	\end{pmatrix}\]
Recordamos que $\ds \begin{cases}
		x' = a(t)x + g(t) \\
		x(t_0) = x_0
	\end{cases} \implies x(t) = x_0 e^{\int_{t_0}^{t} a(s) \odif{s}} + \int_{t_0}^{t} g(s) e^{\int_{s}^{t} a(u) \odif{u}} \odif{s}$.

\begin{teo}
	Sean $t_0 \in (a, b)$ y $X_0 \in \R^d$.
	\[\implies \tex{El PVI }\{X' = A(t)X + g(t) \we X(t_0) = X_0\}\tex{ tiene solución única en }(a, b)\]
	\begin{dem}
		Sean $\alpha, \beta \in \R : a < \alpha < \beta < b$ i.e. $[\alpha, \beta] \subset (a, b)$. Vamos a demostrar existencia y unicidad global en $[\alpha, \beta]$. En efecto, denotamos
		\[\forall t \in (a,b) : \forall X \in \R^d : f(t, X) \defeq A(t)X + g(t)= \begin{pmatrix}
				f^1(t, X) \\
				\vdots    \\
				f^d(t, X)
			\end{pmatrix}\]
		Por otra parte, sea $\ds M  \defeq \max_{\substack{1\leq i \we j \leq d \\ t\in [\alpha, \beta]}} \abs{a_j^i(t)}$.
		\[\begin{aligned}
				\implies \abs{f^i(t, X) - f^i(t, Y)} & = \abs{A(t) (X - Y)} = \abs{\sum_{j=1}^{d} a_j^i(t) (X^j - Y^j)}                            \\
				                                     & \leq \sum_{j=1}^{d} \abs{a^i_j(t)} \abs{X^j - Y^j} \leq M \sum_{j=1}^{d} \abs{X^j - Y^j}    \\
				                                     & \leq M \sqrt{d} \sqrt{\textstyle \sum_{j=1}^{d} \abs{X^j - Y^j}^2} = M \sqrt{d} \abs{X - Y}
			\end{aligned}\]
		\[\abs{f(t, X) - f(t, Y)} \leq \left( \sum_{i=1}^{d} \abs{f^i(t, X) - f^i(t, Y)}^2 \right)^{\frac{1}{2}} \leq \left( \sum_{i=1}^{d} M^2 d \abs{X - Y}^2 \right)^{\frac{1}{2}} = M d \abs{X - Y}\]
		\[\implies \forall X, Y \in \R^d : \forall t \in [\alpha, \beta] : \abs{f(t, X) - f(t, Y)} \leq M d \abs{X - Y}\]
		Por el teorema de existencia y unicidad global, $\exists$ una única solución definida en $[\alpha, \beta]$. Como $\alpha, \beta$ son arbitrarios, la solución es única en $\ds (a, b) = \bigcup_{\varepsilon \in (0, \varepsilon_0)} [a+ \varepsilon, b - \varepsilon]$.

		Definimos $\appl{X}{(a, b)}{\R^d}$ como: si $t \in (a, b)$, tomamos $\varepsilon > 0$ tal que $t \in [a + \varepsilon, b - \varepsilon]$, $X(t)$ es la única solución en $[a + \varepsilon, b - \varepsilon]$.
	\end{dem}
\end{teo}
\subsection{El problema homogéneo}
Sea $X'=A(t)X$ un sistema lineal homogéneo (SLH). Sean $X(t)$ y $Y(t)$ soluciones del SLH y $\alpha, \beta \in \R$. Vemos que las soluciones forman un espacio vectorial (de funciones).
\[(\alpha X + \beta Y)' = \alpha X' + \beta Y' = \alpha A(t)X + \beta A(t)Y = A(t)(\alpha X + \beta Y)\]

\begin{defn}[Independencia lineal de funciones]
	Sean $\appl{X_1, X_2, \ldots, X_k}{(a, b)}{\R^d}$ funciones, son linealmente independientes
	\[\iff \left(\forall t \in (a, b) : \sum_{j=1}^{k} \alpha_i X_i (t) = 0 \implies \alpha_1 = \cdots = \alpha_k = 0 \right)\]
\end{defn}

Si $X_1(\Bar{t}) , \dots , X_k(\Bar{t})$ para algún $\Bar{t} \in (a,b)$, tenemos que las funciones son linealmente independientes:
\[\forall t \in (a,b) : \sum_{i=1}^k \alpha_i X_i(t) = 0 \implies \sum_{i=1}^k \alpha_i X_i(\Bar{t}) = 0 \implies \forall i \implies X_i : \forall i : \alpha_i = 0  \text{ linealmente independiente }\]

\fecha{15/04/2024}

\begin{prop}
	Sean $A(t) = \left(a^i_j(t)\right) \we a^i_j \in \mathcal{C}((a,b)) \we i,j=1,\ldots,d$
	\[\implies \dim{\{\appl{X}{(a,b)}{\R^d} : X'=A(t)X\}} = d\]
	\begin{dem}
		Para cada $j=1, \dots, d$, sea $\Pi_j(t;t_0)$ la solución (única) de
		\[\forall t \in (a,b) : X'=A(t)X \we X(t_0) = e_j\]
		donde $e_j$ es el vector $j$-ésimo de la base canónica de $\R^d$. Sabemos que $\ds \left\{\Pi_j(t_0; t_0)\right\}_{j=1}^d \,(*)$ es linealmente independiente. Por tanto, $\ds \left\{\Pi_j\right\}_{j=1}^d$ es linealmente independiente. \\
		\hspace*{\fill} $(*)$ Se denominan soluciones principales del SLH.

		Veamos ahora que $\ds \left\{\Pi_j\right\}_{j=1}^d$ es un sistema generador de las soluciones del SLH. Sea $X(t)$ una solución del SLH. Denotamos $X_0=X(t_0) \in \R^d$. Escribimos $X_0$ como combinación lineal de la base canónica de $\R^d$: $\ds X_0 = \sum_{j=1}^{d} X_0^j e_j$.

		Definimos la función $\ds \forall t \in (a, b) : Y(t)\defeq \sum_{j=1}^{d} X_0^j \cdot \Pi_j(t;t_0)$. Observamos que
		\begin{itemize}
			\item $Y$ es combinación lineal de soluciones del SLH, luego también es solución del SLH.
			\item $Y(t_0) = \sum_{j=1}^{d} X_0^j e_j = X_0$
		\end{itemize}
		Por unicidad, $\ds X = Y = \sum_{j=1}^{d} X_0^j \Pi_j(t;t_0)$.
	\end{dem}
\end{prop}

Supongamos que tenemos un conjunto $\left\{\phi_1, \dots, \phi_d\right\}$ de soluciones del SLH. ¿Cómo sabemos si son linealmente independientes?
\[\exists \Bar(t) \in (a,b) : \det{\left(\phi_1, \dots, \phi_d\right)} \neq 0 \iff \{\phi_j(\Bar(t))\}_{j=1}^d\tex{ son vectores l.i.}\]

\begin{defn}[Wronskiano]
	Sean $\appl{X_j}{(a,b)}{\R^d}, j=1, \dots, d$ funciones $W\left[X_1, \dots, X_d\right]$ es su wronskiano
	\[\iff \forall t \in (a,b) : W\left[X_1, \dots, X_d\right](t) = \det{\left(X_1(t), \dots, X_d(t)\right)}\]
\end{defn}

Ya sabemos que si $W\left[X_1, \dots, X_d\right](\Bar{t}) \neq 0$ para algún $\Bar{t}\in(a,b)$ entonces $\{X_1, \dots, X_d\}$ son l.i. Sin embargo, si $W\left[X_1, \dots, X_d\right](t)$ se anula en algún punto (o en todos), no podemos afirmar que sean linealmente dependientes.

\begin{ejem}
	Sea $X_1 = \begin{pmatrix}
			t^3 \\
			3t^2
		\end{pmatrix} \we X_2 = \begin{cases}
			\begin{pmatrix}
				-t^3 \\
				-3t^2
			\end{pmatrix} & t \in (-1, 0] \\
			\begin{pmatrix}
					t^3 \\
					3t^2
				\end{pmatrix} & t \in [0, 1)
		\end{cases} \implies X_1, X_2 \in \mathcal{C}((-1, 1), \R^2)$.
	\[\forall t \in (-1, 0] : W\left[X_1, X_2\right](t) = \det{\begin{pmatrix}
					t^3  & -t^3  \\
					3t^2 & -3t^2
				\end{pmatrix}} = 0\]
		\[\forall t \in [0, 1) : W\left[X_1, X_2\right](t) = \det{\begin{pmatrix}
				t^3  & t^3  \\
				3t^2 & 3t^2
			\end{pmatrix}} = 0\]
	Sin embargo, veamos que $X_1, X_2$ son l.i. como funciones.

	Sean $\alpha, \beta \in \R$ tales que $\forall t \in (-1, 1) :\alpha X_1(t) + \beta X_2(t) = 0$. Si $t \in (-1, 0)$, entonces $\alpha X_1(t) + \beta X_2(t) = 0 \implies (\alpha - \beta)X_1(t) = 0 \implies \alpha = \beta$. Si $t \in [0, 1)$, entonces $\alpha X_1(t) + \beta X_2(t) = 0 \implies (\alpha + \beta)X_1(t) = 0 \implies \alpha = -\beta$. Por tanto, $\alpha = \beta = 0$.
\end{ejem}

\begin{lem}
	Sean $X_j, j = 1, \dots, k$ soluciones del SLH.
	\[\exists \, \Bar{t} \in (a, b) : \{X_j(\Bar{t}) : j \in \N_k\} \tex{ l.i.} \implies \{X_j : j \in \N_k\} \tex{ l.i.}\]
	\begin{dem}
		Sabemos que $\exists \alpha_1, \dots, \alpha_k \in \R$ no todos nulos $ : \ds \sum_{j=1}^{k} \alpha_j X_j(\Bar{t}) =0$. \\
		Definimos $\ds \forall t \in (a,b) : Y(t) \defeq \sum_{j=1}^{k} \alpha_j X_j(t)$. Esta función es solución del SLH y $Y(\Bar{t}) = 0$. Por tanto, $Y$ es solución del PVI $\{X' = A(t)X \we X(\Bar{t}) = 0\}$.

		Como la solución es única $\ds \forall t\in (a, b) : Y(t) = 0 \implies \forall t\in (a, b) : \sum_{j=1}^{k} \alpha_j X_j(t) = 0$.\\
		Por definición, $\{X_j : j \in \N_k\}$ son l.d.
	\end{dem}
\end{lem}

\begin{prop}
	Sean $X_1, \dots, X_d$ soluciones del SLH.
	\[\implies \bigg(\forall t \in (a, b) : W\left[X_1, \dots, X_d\right](t) = 0 \niff  \forall t \in (a, b) : W\left[X_1, \dots, X_d\right](t) \neq 0\bigg)\]
\end{prop}

\fecha{16/04/2024}

\begin{defn}[Sistema fundamental]
	Sean $X'=A(t)X$ un SLH, $\{\phi_1, \dots, \phi_d\}$ es un sistema fundamental de soluciones $\iff \phi_1, \dots, \phi_d\tex{ son soluciones del SLH l.i.}$
\end{defn}

Un sistema fundamental define una \underline{matriz solución fundamental}: $\ds \Phi(t) \defeq \begin{pmatrix}
		\phi_1(t) & \cdots & \phi_d(t)
	\end{pmatrix}$.

A partir de $\Phi$ podemos encontrar cualquier solución de $\{X'=A(t)X \we X(t_0) = X_0\}$:
\[X=\Phi c = \begin{pmatrix}
		\phi_1 & \cdots & \phi_d
	\end{pmatrix} \begin{pmatrix}
		c_1    \\
		\vdots \\
		c_d
	\end{pmatrix} = \sum_{j=1}^{d} c_j \phi_j \tex{ con }c \in \R^d \we X(t_0) = \Phi(t_0) = c=X_0 \]
\[\implies c = \left(\Phi(t_0)\right)^{-1} X_0 \implies \boxed{X(t) = \Phi(t) \left(\Phi(t_0)\right)^{-1} X_0}\]

\subsection{Sistema no homogéneo}

\begin{prop}
	Sea $\forall t \in (a,b) \subset \R : X'=A(t)X + g(t)$ un sistema lineal (SL).\\
	Sea $X_p$ una sol. particular del SL y sea $\{X_1, \dots, X_d\}$ una base del espacio de sol. del SLH.
	\[\implies X \tex{ sol. del SL} \iff \exists \{\alpha_i\}_{i=1}^d \subset \R :\forall t \in (a, b) : X(t) = X_p(t) + \sum_{i=1}^{d} \alpha_i X_i(t)\]
	\begin{dem} % TODO: completar detalles
		El cambio de variable $Y = X - X_p$ transforma el SL en un SLH. Es decir, el conjunto de soluciones del SL se caracteriza mediante el del SLH.
	\end{dem}
\end{prop}

\allbold{¿Cómo encontrar una solución particular $X_p$ del SL?}

Si $\Phi$ es una matriz solución fundamental del SLH, buscamos $X_p$ de la forma $X_p = \Phi(t) \alpha(t)$, donde $\alpha(t) = \left(\alpha^1(t), \dots, \alpha^d(t)\right)$
\[\begin{aligned}
		\implies X_p' & = \Phi'(t) \alpha(t) + \Phi(t) \alpha'(t) = A(t) \Phi(t) \alpha(t) + \Phi(t) \alpha'(t) = A(t) X_p + g(t)              \\
		              & = A(t) \Phi(t) \alpha(t) + g(t) \implies \Phi(t)\alpha'(t) = g(t) \implies \alpha'(t) = \left(\Phi(t)\right)^{-1} g(t)
	\end{aligned}\]
Por tanto, $\ds \alpha(t) = \int_{t_0}^{t} \left(\Phi(s)\right)^{-1} g(s) \odif{s}$. En definitiva, $\ds X_p(t) = \Phi(t) \int_{t_0}^{t} \left(\Phi(s)\right)^{-1} g(s) \odif{s}$

Como toda solución del SL es de la forma $\ds X(t) = X_p(t) + \sum_{i=1}^{d} \alpha_i X_i(t)$, podemos escribir
\[X(t) = \Phi(t) \left(c + \int_{t_0}^{t} \left(\Phi(s)\right)^{-1} g(s) \odif{s}\right) \tex{ con } c \in \R^d\]
Para resolver el PVI, basta con calcular $c$ tal que $X(t_0) = X_0$.
\[X_0 = X(t_0) = \Phi(t_0) \left(c + \int_{t_0}^{t_0} \left(\Phi(s)\right)^{-1} g(s) \odif{s}\right) = \Phi(t_0) c \implies c = \left(\Phi(t_0)\right)^{-1} X_0\]
\[\implies \forall t \in (a, b) : \boxed{X(t) = \Phi(t) \left(\left(\Phi(t_0)\right)^{-1} X_0 + \int_{t_0}^{t} \left(\Phi(s)\right)^{-1} g(s) \odif{s}\right)}\]

\fecha{17/04/2024}

\subsection{Sistemas lineales de coeficientes constantes}

Sea $\forall t \in \R : X'=AX + g(t)$, con $A =\left(a_i^j\right)_{i,j=1}^d$ con $a_i^j \in \R$ constantes y $g \in \mathcal{C}(\R, \R^d)$.

Nos reducimos al caso $g\equiv 0$.
\subsubsection{El caso diagonalizable}

\allbold{Diagonalizable con autovalores reales}

Si $d=1$, entonces $A = a \in \R$ y $X' = aX \implies X(t) = c e^{at}, c\in \R$.

Si $d>1$, entonces ¿$X(t)=\xi e^{\lambda t}$ con $t \in \R$ y $\xi \ne 0 \in \R^d$?
\[X'(t) = \lambda e^{\lambda t} \xi \implies AX(t) = e^{\lambda t} A\xi\]
Para que $X$ sea solución, $\lambda e^{\lambda t} \xi = e^{\lambda t} A\xi \implies e^{\lambda t} \left(A\xi - \lambda \xi I\right) = 0 \implies A\xi = \lambda \xi I$.

Si los autovalores de $A$ $\lambda_i$ son reales y existe una base de autovectores asociados $\xi_i\in\R^d$, entonces las funciones $\ds \left\{X_i(t) = \xi_i e^{\lambda_i t}\right\}_{i=1}^d$ forman un sistema fundamental de soluciones del SLH (porque $\{X_i(0)\}_{i=1}^d$ son independientes).

Por tanto, la solución general del SLH es $\ds \boxed{X(t) = \sum_{i=1}^d c_i X_i(t) =  \sum_{i=1}^{d} c_i \xi_i e^{\lambda_i t}}$.

\begin{ejem}
	Sea $\begin{cases}
			(X^1)' =   X^1 + 2 X^2 \\
			(X^2)' = 2 X^1 +  X^2  \\
		\end{cases} \iff X' = \begin{pmatrix}
			1 & 2 \\
			2 & 1
		\end{pmatrix} X$.
	\[\begin{vmatrix}
			1-\lambda & 2         \\
			2         & 1-\lambda
		\end{vmatrix} = 0 \implies \lambda^2 - 2\lambda - 3 = 0 \implies \lambda_1 = 3, \lambda_2 = -1 \implies \Lambda = \{-1, 3\}\]
	\begin{itemize}
		\item $\lambda_1 = 3$ : $\begin{pmatrix}
				      -2 & 2  \\
				      2  & -2
			      \end{pmatrix} \begin{pmatrix}
				      \xi_1 \\
				      \xi_2
			      \end{pmatrix} = \begin{pmatrix}
				      0 \\
				      0
			      \end{pmatrix} \implies \xi_1 = \begin{pmatrix}
				      1 \\
				      1
			      \end{pmatrix} \implies X_1(t) = \begin{pmatrix}
				      1 \\
				      1
			      \end{pmatrix} e^{3t}$.
		\item $\lambda_2 = -1$ : $\begin{pmatrix}
				      2 & 2 \\
				      2 & 2
			      \end{pmatrix} \begin{pmatrix}
				      \xi_1 \\
				      \xi_2
			      \end{pmatrix} = \begin{pmatrix}
				      0 \\
				      0
			      \end{pmatrix} \implies \xi_2 = \begin{pmatrix}
				      1 \\
				      -1
			      \end{pmatrix} \implies X_2(t) = \begin{pmatrix}
				      1 \\
				      -1
			      \end{pmatrix} e^{-t}$.
	\end{itemize}

	Como $\{X_1(0), X_2(0)\}$ son l.i., $\{X_1, X_2\}$ es un sistema fundamental de soluciones del SLH. Por tanto, la solución general del SLH es
	\[X(t) = c_1 \begin{pmatrix}
			1 \\
			1
		\end{pmatrix} e^{3t} + c_2 \begin{pmatrix}
			1 \\
			-1
		\end{pmatrix} e^{-t} = \begin{pmatrix}
			c_1 e^{3t} + c_2 e^{-t} \\
			c_1 e^{3t} - c_2 e^{-t}
		\end{pmatrix} = {\begin{pmatrix}
			e^{3t} & e^{-t}  \\
			e^{3t} & -e^{-t}
		\end{pmatrix}} \begin{pmatrix}
			c_1 \\
			c_2
		\end{pmatrix}\]

	Dado el PVI $\begin{cases}
			X' = AX \\
			X(0) = (1, 0) \eqdef e_1
		\end{cases}$queremos calcular $c_1, c_2$ tal que $X(0) = \Phi(0) c = c$.
	\[\begin{pmatrix}
			1 \\
			0
		\end{pmatrix} = X(0) = \begin{pmatrix}
			1 & 1  \\
			1 & -1
		\end{pmatrix} \begin{pmatrix}
			c_1 \\
			c_2
		\end{pmatrix} = \begin{pmatrix}
			c_1 + c_2 \\
			c_1 - c_2
		\end{pmatrix} \implies c_1 = c_2 = \frac{1}{2}\]
\end{ejem}

\begin{ejem}
	Sea el SL $X' = \begin{pmatrix}
			1 & 2 \\
			2 & 1
		\end{pmatrix} X + \begin{pmatrix}
			t \\
			e^t
		\end{pmatrix}$.
	\[X(t) = X_p + \Phi(t) c \we X_p(t) = \Phi(t) \int_{t_0}^{t} \left(\Phi(s)\right)^{-1} g(s) \odif{s}\]
	\[\implies X_p = \frac{1}{2} \begin{pmatrix}
			e^{3t} & e^{-t}  \\
			e^{3t} & -e^{-t}
		\end{pmatrix} \int_{0}^{t} \begin{pmatrix}
			e^{-3s} & e^{3s} \\
			e^{s}   & -e^{s}
		\end{pmatrix} \begin{pmatrix}
			s \\
			e^s
		\end{pmatrix} \odif{s} = \frac{1}{2} \begin{pmatrix}
			e^{3t} & e^{-t}  \\
			e^{3t} & -e^{-t}
		\end{pmatrix} \int_{0}^{t} \begin{pmatrix}
			s e^{-3s} + e^{-2s} \\
			s e^{s} - e^{2s}    \\
		\end{pmatrix} \odif{s}\]
	\[\implies X_p = \frac{1}{2} \begin{pmatrix}
			e^{3t} & e^{-t}  \\
			e^{3t} & -e^{-t}
		\end{pmatrix} \begin{pmatrix}
			\int_{0}^{t} s e^{-3s} + e^{-2s} \odif{s} \\
			\int_{0}^{t} s e^{s} - e^{2s} \odif{s}
		\end{pmatrix} = \cdots = c_1 \begin{pmatrix}
			e^{3t} \\
			e^{3t}
		\end{pmatrix} + c_2 \begin{pmatrix}
			e^{-t} \\
			-e^{-t}
		\end{pmatrix}\]
\end{ejem}

\fecha{18/04/2024}

\allbold{Diagonalizable con autovalores complejos}

Partimos de una base de autovectores en $C^d$. A partir de ella, construimos una en $\R^d$.

Sea $\lambda = \alpha + \beta i$ autovalor de $A$ y $\xi = u + i v$, con $u, v \in \R^d$, autovector asociado. Observamos que $A\xi = \lambda \xi \implies A \bar{\xi} = \bar{\lambda} \bar{\xi}$.

Podemos definir dos soluciones complejas del SLH linealmente independientes:
\[X_1(t) = e^{\lambda t} \xi \quad \we \quad X_2(t) = e^{\overline{\lambda} t} \overline{\xi} = \overline{X_1(t)}\]
Ahora, tomamos las combinaciones lineales $\ds X_R(t) =\frac{X_1(t) + X_2(t)}{2}$ y $\ds X_I(t) = \frac{X_1(t) - X_2(t)}{2i}$, que son soluciones (por ser combinaciones lineales de soluciones) reales del SLH.

Veamos que son independientes: Sean $c_1, c_2 \in \R$ tales que $\forall t \in \R: c_1 X_R(t) + c_2 X_I(t) = 0$.
\[\frac{c_1}{2} \left(X_1 + X_2\right) + \frac{c_2}{2i} \left(X_1 - X_2\right) = 0 \implies \left(\frac{c_1}{2} + \frac{c_2}{2i}\right)X_1 + \left(\frac{c_1}{2} - \frac{c_2}{2i}\right)X_2 = 0\]
Como $X_1$ y $X_2$ son l.i., $\ds \frac{c_1}{2} + \frac{c_2}{2i} = 0 = \frac{c_1}{2} - \frac{c_2}{2i} \implies c_1 = c_2 = 0$.

\begin{ejem}
	Sea el SLH $X' = \begin{pmatrix}
			1 & 0  & -1 \\
			1 & 0  & 0  \\
			1 & -1 & 0
		\end{pmatrix} X = AX$.
	\[\det(A -\lambda I) = \begin{vmatrix}
			1-\lambda & 0        & -1       \\
			1         & -\lambda & 0        \\
			1         & -1       & -\lambda
		\end{vmatrix} = 0 \implies \Lambda = \{1, i, -i\}\]
	\begin{itemize}
		\item $\lambda_1 = 1$ : $\begin{pmatrix}
				      0 & 0  & -1 \\
				      1 & -1 & 0  \\
				      1 & -1 & -1
			      \end{pmatrix} \begin{pmatrix}
				      \xi_1^1 \\
				      \xi_1^2 \\
				      \xi_1^3
			      \end{pmatrix} = \begin{pmatrix}
				      0 \\
				      0 \\
				      0
			      \end{pmatrix} \implies \xi_1 = \begin{pmatrix}
				      1 \\
				      1 \\
				      0
			      \end{pmatrix} \implies X_1(t) = \begin{pmatrix}
				      1 \\
				      1 \\
				      0
			      \end{pmatrix} e^t$.
		\item $\lambda_2 = i$ : $\begin{pmatrix}
				      1-i & 0  & -1 \\
				      1   & -i & 0  \\
				      1   & -1 & -i
			      \end{pmatrix} \begin{pmatrix}
				      \xi_2^1 \\
				      \xi_2^2 \\
				      \xi_2^3
			      \end{pmatrix} = \begin{pmatrix}
				      0 \\
				      0 \\
				      0
			      \end{pmatrix} \implies \begin{cases}
				      \xi_2 = (i, 1, 1+i)             \\
				      \overline{\xi_1} = (-i, 1, 1-i) \\
			      \end{cases}$.
		      \[\implies \begin{cases}
				      X_2(t) = \xi_2 e^{it} = (i, 1, 1+i) e^{it} \\
				      X_3(t) = \overline{\xi_2} e^{-it} = (-i, 1, 1-i) e^{-it}
			      \end{cases}\]
		      \[X_2(t) = \begin{pmatrix}
				      -\sin t \\
				      \cos t  \\
				      \cos t - \sin t
			      \end{pmatrix} + i \begin{pmatrix}
				      \cos{t} \\
				      \sin{t} \\
				      \cos{t} + \sin{t}
			      \end{pmatrix} \we X_3(t) = \begin{pmatrix}
				      -\sin{t} \\
				      \cos{t}  \\
				      \cos{t} - \sin{t}
			      \end{pmatrix} - i \begin{pmatrix}
				      \cos{t} \\
				      \sin{t} \\
				      \cos{t} + \sin{t}
			      \end{pmatrix}\]
		      Definimos $X_R(t) = \frac{X_2(t) + X_3(t)}{2}$ y $X_I(t) = \frac{X_2(t) - X_3(t)}{2i}$.
		      \[\implies X_R(t) = \begin{pmatrix}
				      -\sin{t} \\
				      \cos{t}  \\
				      \cos{t} - \sin{t}
			      \end{pmatrix} \we X_I(t) = \begin{pmatrix}
				      \cos{t} \\
				      \sin{t} \\
				      \cos{t} + \sin{t}
			      \end{pmatrix}\]
		      Por tanto, la solución general del SLH es
		      \[X(t) = c_1 \begin{pmatrix}
				      1 \\
				      1 \\
				      0
			      \end{pmatrix} e^t + c_2 \begin{pmatrix}
				      -\sin{t} \\
				      \cos{t}  \\
				      \cos{t} - \sin{t}
			      \end{pmatrix} + c_3 \begin{pmatrix}
				      \cos{t} \\
				      \sin{t} \\
				      \cos{t} + \sin{t}
			      \end{pmatrix}\]
	\end{itemize}
\end{ejem}

\subsubsection{El caso no diagonalizable}

Sea el PVI $\left\{X' = AX \we X(0) = 0\right\}$ con $A \in \mathcal{M}_d(\R)$ y $A$ no diagonalizable. Definimos las iteraciones de Picard:
\[\begin{aligned}
		X_0(t)  & = X_0                                                                                                                                                         \\
		X_1 (t) & = X_0 +\int_0^t A X_0 \odif{s} = X_0 + AX_0t = (I + At)X_0                                                                                                    \\
		X_2 (t) & = X_0 + \int_0^{t} AX_1(s) \odif{s} = X_0 + \int_0^{t} AX_0 + A^2 X_0 s \odif{s} = X_0 + AX_0 t + \frac{1}{2} A^2 X_0 t^2 = (I + At + \frac{1}{2} A^2 t^2)X_0
	\end{aligned}\]
Como vemos $\ds X_k(t) = \left(\sum_{j=0}^k A^j \frac{t^j}{j!}\right)X_0 \xrightarrow{k\to \infty} \left(\sum_{j=0}^{\infty} A^j \frac{t^j}{j!}\right)X_0$.

\begin{defn}[Exponencial de una matriz]
	Sea $A \in \mathcal{M}_d(\R)$, $e^A$ es su exponencial
	\[\iff e^A \defeq \sum_{j=0}^{\infty} A^j \frac{t^j}{j!}\]
\end{defn}

\fecha{22/04/2024}

\begin{obs}
	Usando las iteradas de Picard, se comprueba que $X(t) = e^{At}X_0$ es la única solución de $X' = AX \we X(0) = X_0$.
\end{obs}

\allbold{Propiedades del exponencial de una matriz:} Sean $A, B \in \mathcal{M}_d(\R)$ matrices cuadradas.
\begin{enumerate}
	\item Si $AB = BA$, entonces $e^{A+B} = e^A e^B$.
	\item Si $\alpha, \beta \in \R$, entonces $e^{(\alpha + \beta)A} = e^{\alpha A} e^{\beta A}$.
	\item $\lambda \in \R$ autovalor de $A \iff e^{\lambda} \in \R$ autovalor de $e^A$.
	\item $\det{(e^A)} = e^{\trz{A}}$ (Junto con $\det(A) = \lambda_1 \cdots \lambda_d$ y $\trz{A} = \lambda_1 + \cdots + \lambda_d$).
	\item $e^{A}$ es invertible con $e^{-A} = \left(e^A\right)^{-1}$.
	\item Si $C$ es una matriz invertible tal que $A=CJC^{-1}$, entonces $e^A = Ce^JC^{-1}$.
	\item Si en 6., $J$ es una matriz por bloques, entonces $e^J$ es también una matriz por bloques.
	      \[J=\begin{pmatrix}
			      J_1    & \dots  & 0      \\
			      \vdots & \ddots & \vdots \\
			      0      & \dots  & J_k
		      \end{pmatrix} \implies e^J = \begin{pmatrix}
			      e^{J_1} & \dots  & 0       \\
			      \vdots  & \ddots & \vdots  \\
			      0       & \dots  & e^{J_k}
		      \end{pmatrix}\]
\end{enumerate}

\begin{defn}[Volumen]
	Sea $E\subset\R^d$ un conjunto ``medible'', $\ds \vol{E} \defeq \int_{E} 1 \odif{x}$.
\end{defn}

\begin{cor}[Liouville]
	Sea $E \subset \R^d$ un conjunto medible y $A \in \mathcal{M}_d(\R)$.
	\[\implies \forall t \in \R : \vol{e^{tA}(E)} = e^{t\trz{A}} \vol{E}\]
	En particular, si $\trz{A} =0$ y, por tanto, $\trz{tA} = 0$, entonces $\forall t \in \R : \vol{e^{tA}(E)} = \vol{E}$. En palabras, la evolución mantiene el volumen.
	\begin{dem}
		Definimos $\appl{\Phi}{\R^d}{\R^d}$ como $\Phi(\xi) = e^{tA}\xi$.
		\[\begin{aligned}
				\implies \vol{\left(e^{tA} (E)\right)} & = \int_{e^{tA}(E)} 1 \odif{x} = \int_{\Phi(E)} 1 \odif{x} = \int_{E} \left|\det{\operatorname{J}_\Phi(y)}\right| \odif{y}                    \\
				                                       & = \int_{E} \left|\det{e^{tA}}\right| \odif{y} = \abs{\det{e^{tA}}} \int_{E} 1 \odif{y} = \abs{e^{t\trz{A}}} \vol{E} = e^{t\trz{A}} \vol{(E)}
			\end{aligned}\]
	\end{dem}
\end{cor}

\begin{teo}[Forma canónica de Jordan]
	Sea $A \in \mathcal{M}_d(\mathbb{C})$, entonces $\exists$ un cambio lineal de coordenadas $C$ tal que $A$ se transforma en una matriz por bloques:
	\[C^{-1} A C = \begin{pmatrix}
			J_1    & \dots  & 0      \\
			\vdots & \ddots & \vdots \\
			0      & \dots  & J_k
		\end{pmatrix} \tex{ con } J_i = \alpha_i I + N_i\]
	donde $\alpha_i$ son los autovalores de $A$ y $N_i$ es una matriz cuadrada con unos en la diagonal por encima de la principal, y lo demás son ceros.
\end{teo}

\fecha{23/04/2024}
\[e^{J_i} = e^{\alpha_i I + N_i} = e^{\alpha_i} e^{N_i} \tex{ porque } e^{\lambda I} = \sum_{j=0}^{\infty} \frac{\lambda^j}{j!} I = \left(\sum_{j=0}^{\infty} \frac{\lambda^j}{j!}\right) I = e^{\lambda} I\]
Además $N_i^k = 0$ para algún $k \in \N$. Por tanto, $\ds e^{N_i} = \sum_{j=0}^{\infty} \frac{N_i^j}{j!} = \sum_{j=0}^{k-1} \frac{N_i^j}{j!}$
\[\implies e^{J_i} = e^{\alpha_i} e^{N_i} = e^{\alpha_i} \sum_{j=0}^{k-1} \frac{N_i^j}{j!} = e^{\alpha_i} \begin{pmatrix}
		1      & 1      & \frac{1}{2!} & \cdots & \frac{1}{(k-1)!} \\
		0      & 1      & 1            & \cdots & \frac{1}{(k-2)!} \\
		0      & 0      & 1            & \cdots & \frac{1}{(k-3)!} \\
		\vdots & \vdots & \vdots       & \ddots & \vdots           \\
		0      & 0      & 0            & \cdots & 1
	\end{pmatrix}\]
Sin embargo, lo que nos interesa es $e^{tA} = Ce^{J}C^{-1}$.
\[\implies e^{tA} = C \begin{pmatrix}
		e^{t J_1} & \dots  & 0         \\
		\vdots    & \ddots & \vdots    \\
		0         & \dots  & e^{t J_k}
	\end{pmatrix} C^{-1} = \begin{pmatrix}
		e^{t J_1} & \dots  & 0         \\
		\vdots    & \ddots & \vdots    \\
		0         & \dots  & e^{t J_k}
	\end{pmatrix}\]
\[\implies e^{t J_i} = e^{t\alpha_i}e^{tN_i} = e^{t\alpha_i} \sum_{j=0}^{k-1} \frac{t^j}{j!} N_i^j = e^{t\alpha_i} \begin{pmatrix}
		1      & t      & \frac{t^2}{2!} & \cdots & \frac{t^{k-1}}{(k-1)!} \\
		0      & 1      & t              & \cdots & \frac{t^{k-2}}{(k-2)!} \\
		0      & 0      & 1              & \cdots & \frac{t^{k-3}}{(k-3)!} \\
		\vdots & \vdots & \vdots         & \ddots & \vdots                 \\
		0      & 0      & 0              & \cdots & 1
	\end{pmatrix}\]
En definitiva, la solución general del SLH $X' = AX$ es
\[X(t) = (Ce^{tJ}C^{-1})X_0 \tex{ con } X_0 \in \R^d \tex{ y } e^{tJ} \tex{ matriz por bloques } e^{tJ_i} \tex{ como arriba}\]

\begin{ejem}
	Sea el SLH $X' = A X$ con $A = \begin{pmatrix}
			3 & -4 \\
			1 & -1
		\end{pmatrix}$. El único autovalor de $A$, que no es diagonalizable, es $\lambda = 1$. Se propone la forma de Jordan dada por:
	\[A = CJC^{-1} = \begin{pmatrix}
			2 & 1 \\
			1 & 0
		\end{pmatrix} \begin{pmatrix}
			1 & 1 \\
			0 & 1
		\end{pmatrix} \begin{pmatrix}
			0 & 1  \\
			1 & -2
		\end{pmatrix} \implies e^{t J} = e^{t}\begin{pmatrix}
			1 & t \\
			0 & 1
		\end{pmatrix}\]
	\[\implies X(t) = \begin{pmatrix}
			2 & 1 \\
			1 & 0
		\end{pmatrix} e^t \begin{pmatrix}
			1 & t \\
			0 & 1
		\end{pmatrix} \begin{pmatrix}
			0 & 1  \\
			1 & -2
		\end{pmatrix} X_0, \tex{ con } X_0 \in \R^d\]
	\[\implies X(t) = e^t \begin{pmatrix}
			2t +1 & -4t  \\
			t     & 1-2t
		\end{pmatrix} X_0 \implies \begin{cases}
			X^1(t) = (2t+1)e^t X_0^1 - 4te^t X_0^2 \\
			X^2(t) = te^t X_0^1 + (1-2t)e^t X_0^2
		\end{cases}\]
\end{ejem}

\subsection{Apéndice: EDOs escalares lineales de orden superior}

Sea $x^{(d)} = a_1(t) x + a_2(t) x' + \dots + a_d(t) x^{(d-1)}$, con $\appl{a_1, \dots, a_{n-1}}{(a,b)}{\R}$ continuas, una ecuación diferencial ordinaria (EDO) de orden $d \in \R$.

Mediante el cambio de variables $y^1 = x, y^2 = x', \dots, y^d = x^{(d-1)}$, podemos reescribir la EDO como un sistema de EDOs de primer orden:
\[\begin{cases}
		(y^)1' = y^2 \\
		(y^2)' = y^3 \\
		\vdots       \\
		(y^d)' = a_1(t) y^1 + \dots + a_d(t) y^d
	\end{cases} \iff Y'(t) = AY \tex{ donde } A= \begin{pmatrix}
		0      & 1      & 0      & \dots  & 0      \\
		0      & 0      & 1      & \dots  & 0      \\
		\vdots & \vdots & \vdots & \ddots & \vdots \\
		0      & 0      & 0      & \dots  & 1      \\
		a_1(t) & a_2(t) & a_3(t) & \dots  & a_d(t)
	\end{pmatrix}\]

