\fecha{12/03/2024}
\section{Variables aleatorias continuas}
Hasta ahora en $(\Omega, \F, P)$, una variable aleatoria $X$ discreta era una función $\appl{X}{\Omega}{\R}$ tal que $\exists N \subseteq \N : \abs{X(\Omega)}=\abs{N}$ y $P(X=k)=P(X^{-1}(k))$.
\begin{defn}[Variable aleatoria]
	Sea $(\Omega, \F, P)$ un espacio de probabilidad, la función $\appl{X}{\Omega}{\R}$ es una variable aleatoria $\iff \forall x \in \R : \{\omega\in\Omega : X(\omega) \leq x\} \in \F$
\end{defn}
\begin{prop}
	$X$ es v.a.d. $\implies X$ es variable aleatoria.
	\begin{dem}
		Puedo describir el suceso $\{X\leq x\}$ como unión numerable de sucesos
		\[\{\omega \in\Omega : X(\omega) \leq x\} = \bigcup_{y\in X(\Omega)} \{\omega \in \Omega : X(\omega) = y\}\]
		Como la unión numerable de sucesos es un suceso, $\{X\leq x\} \in \F$.
	\end{dem}
\end{prop}
\begin{defn}[Función de distribución]
	Sea $X$ una variable aleatoria, $\appl{F_X}{\R}{[0,1]}$ es su función de distribución $\iff \forall x \in X(\Omega) : \boxed{F_X(x) = P(X\leq x)}$
\end{defn}
Sea $X$ una v.a.d. que toma los valores $x_1, x_2, \cdots$ con probabilidades $p_1, p_2, \dots$ y con función de masa $p_X \ds \implies F_X(x) = \sum_{x_i \leq x} P(X=x_i)$. Es decir, es la función de masa acumulada.
\begin{lem}
	En $(\Omega, \F, P)$, sea $X$ una variable aleatoria con función de distribución $F_X$.
	\[\implies \begin{aligned}
			1. \quad & \lim_{x\to\infty} F_X(x) = 1 \we \lim_{x\to-\infty} F_X(x) = 0 \\
			2. \quad & F_X \text{ es no decreciente}                                  \\
			3. \quad & F_X \text{ es continua por la derecha}
		\end{aligned}\]
	\begin{dem}
		\begin{enumerate}[topsep=1pt, itemsep=1pt,parsep=3pt] \item[]
			\item Definimos $A_n \defeq \{\omega \in \Omega : X(\omega) \leq n\}$ que es creciente según $n\to \infty$.
			      \[\implies \lim_{n\to\infty} F_X(n) = \lim_{n\to\infty} P(X\leq n) = \lim_{n\to\infty} P\left(\bigcup_{j=1}^n A_j\right) = P\left(\bigcup_{n=1}^\infty A_n\right) = P(\Omega) = 1\]
			\item $x<y \implies \{X\leq x\} \subseteq \{X\leq y\} \implies F_X(x) \leq F_X(y)$.
			\item Definimos $A_h \defeq \{\omega \in \Omega : X(\omega) \leq x + h\}$.
			      \[\implies \lim_{h\to 0^+} F_X(x+h) = \lim_{h\to 0^+} P\left(A_h\right) = P\left(\lim_{h\to 0^+} A_h\right) = P(A_0) = P(X \leq x) = F_X(x)\]
		\end{enumerate}
	\end{dem}
\end{lem}
\begin{teo}
	Sea $\appl{F}{U \subset \R}{[0, 1]}$ una función que cumpla los puntos del lema anterior.
	\[\implies \exists! X \text{ variable aleatoria } : F_X = F\]
	\begin{dem}
		Suponemos que $\exists \appl{X, Y}{\Omega}{\R}$ v.a. tales que $F_X = F = F_Y$.
		\[\implies \forall x \in X(\Omega) = U = Y(\Omega) : P(X \leq x) = F_X(x) = F(x) = F_Y(x) = P(Y \leq x)\]
		Por tanto, $\forall x \in U : P(X\leq x) = P(Y\leq x) \implies X = Y$.
	\end{dem}
\end{teo}
\textbf{Moraleja:} Una variable aleatoria queda determinada por su función de distribución.
\fecha{13/03/2024}
Sea $X$ una variable aleatoria con función de distribución $F_X$ y $a,b\in \R$.
\[\implies P(a<X\leq b)=P(\{x\leq b\}\setminus \{x\leq a\}) = P(x\leq b) - P(x\leq a) = F_X(b)-F_X(a)\]
\vspace{-0.8cm}
\begin{defn}[Variable aleatoria continua]
	Sea $X$ una variable aleatoria con función de distribución $F_X$, $X$ es continua (v.a.c.)
	\[\iff \exists \appl{f_X}{\R}{\R} \geq 0 : \left(F_X(x) = P(X\leq x)=\int_{-\infty}^{x} f_X(y) \odif{y}\right) \we \left(\int_{-\infty}^{\infty} f_X(y) \odif{y}=1\right)\]
	$f_X$ se denomina la \textbf{función de densidad} de $X$.
\end{defn}

\begin{obs}
	\begin{enumerate}[topsep=1pt, itemsep=1pt,parsep=3pt]
		\item[]
		\item $\forall a \in \R : P(X=a) = 0$
		      \begin{dem}
			      Por continuidad de la probabilidad:
			      \[\begin{aligned}
					      \lbox{P(X=a)} & =\lim_{h \to 0^+} P(a-h<X\leq a+h) = \lim_{h\to0^+} F_X(a+h)-F_X(a-h)                                                                                                  \\
					                    & = \lim_{h \to 0^+} \left(\int_{-\infty}^{a} f_X(y) \odif{y} - \int_{-\infty}^{a-h} f_X(y) \odif{y}\right) = \lim_{h\to0^+} \int_{a-h}^{a+h} f_X(y) \odif{y} = \rbox{0}
				      \end{aligned}\]
		      \end{dem}
		\item Cálculo de probabilidades: $\ds\forall a \leq b : P(a\leq X\leq b) = \int_{a}^{b} f_X(y) \odif{y}$
		\item $\ds \forall x \in \R : f_X(x) = \begin{cases}
				      \ds \odv{}{x} F_X(x), & \text{ si } F_X \text{ es derivable en } x \\
				      \ds \quad 0,          & \text{ en otro caso}
			      \end{cases}$
	\end{enumerate}
\end{obs}

\begin{ejem}
	\begin{enumerate}
		\item[]
		\item Para cualquier $f\geq 0$ tal que $\ds \int_{-\infty}^{\infty} f(x) \odif{x} = c \in \R$ tenemos una v.a.c.
		\item $\ds X\sim\U{0,1} \iff f_X(u)=\begin{cases}
				      1, & \text{ si } u\in[0,1] \\
				      0, & \text{ en otro caso}
			      \end{cases} \implies F_X(u) = \begin{cases}
				      0, & \text{ si } u<0       \\
				      u, & \text{ si } u\in[0,1] \\
				      1, & \text{ si } u>1
			      \end{cases}$
		\item $\ds X\sim\EXP{\lambda} \iff f_X(x) = \begin{cases}
				      \lambda e^{-\lambda x}, & \text{ si } x\geq 0  \\
				      0,                      & \text{ en otro caso}
			      \end{cases} = \lambda e^{-\lambda x} \cdot \mathbbm{1}_{\{x\geq 0\}}(x)$
		      \[\implies \forall x > 0 : F_X(x) = \int_{0}^{x} \lambda e^{-\lambda y} \odif{y} = \left[-e^{-\lambda y}\right]_{0}^{x} = 1-e^{-\lambda x} \implies \lim_{x\to\infty} F_X(x) = 1\]
		\item $\ds X\sim\normal{\mu, \sigma^2} \iff f_X(x) = \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$ \\
		      La guay es $\ds X\sim\normal{0, 1} \iff \phi(x) \defeq \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}$. Veamos que $\ds \int_{-\infty}^{\infty} \phi(x) \odif{x} = 1$.
		      \begin{dem}
			      \[I \defeq \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} \odif{x}\implies I^2 = \left(\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}\odif{x}\right)\left(\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{y^2}{2}} \odif{y}\right)\]
			      \[\begin{aligned} \implies I^2 & = \frac{1}{2\pi} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} e^{-\frac{1}{2}\left(x^2+y^2\right)} \odif{x} \odif{y} = \frac{1}{2\pi} \int_{0}^{2\pi} \int_{0}^{\infty} e^{-\frac{r^2}{2}} r \odif{r} \odif{\theta} \\
                             & = \frac{1}{2\pi} \int_{0}^{2\pi} \left[-e^{-\frac{r^2}{2}}\right]_{0}^{\infty} \odif{\theta} = \frac{1}{2\pi} \int_{0}^{2\pi} 1 \odif{\theta} = 1
				      \end{aligned}\]
		      \end{dem}
	\end{enumerate}
\end{ejem}

\fecha{18/03/2024}
\subsection{Funciones / Transformaciones de v.a.c.}
Sea $X$ una v.a.c. con función de densidad $f_X$ y $\appl{g}{X(\Omega)}{\R}$ una función real y $Y\defeq g(X)$:
\begin{itemize}
	\item $Y$ es variable aleatoria $\iff \forall y \in \R : \{\omega \in \Omega : g(X(\omega)) \leq y\} \in \F$.\\
	      Esto es cierto para las $g$ ``habituales'' (continuas, monótonas, etc.), para más detalle, hay que esperar a teoría de la medida.
	\item ¡Cuidado! $Y$ puede no ser continua.
	\item $Y$ v.a. $\implies Y$ tiene función de distribución $F_Y(y) = P(Y\leq y) = P(g(X)\leq y) = \, ???$
	      \begin{ejem}
		      $g(x) = ax + b$ con $a\ne 0 \we b \in \R$.
		      \[\implies F_Y(y)= P(aX+b\leq y) = \begin{cases}
				      P\left(X \leq \frac{y-b}{a}\right),                                       & \text{ si } a>0 \\
				      P\left(X \geq \frac{y-b}{a}\right) = 1 - P\left(X < \frac{y-b}{a}\right), & \text{ si } a<0
			      \end{cases}\]
		      Para $X$ v.a. continua, $\ds F_Y(y) = \begin{cases}
				      F_X\left(\frac{y-b}{a}\right),  & \text{ si } a>0 \\
				      1-F_X\left(\frac{y-b}{a}\right) & \text{ si } a<0
			      \end{cases}$.
		      \[\implies f_Y(y) =\odv{}{y} F_Y(y) = \begin{cases}
				      \frac{1}{a} f_X\left(\frac{y-b}{a}\right),  & \text{ si } a>0 \\
				      -\frac{1}{a} f_X\left(\frac{y-b}{a}\right), & \text{ si } a<0
			      \end{cases} = \frac{1}{\abs{a}} f_X\left(\frac{y-b}{a}\right)\]
	      \end{ejem}
\end{itemize}

\begin{teo}
	En $(\Omega, \F, P)$, sea $X$ una v.a.c. con función de densidad $f_X \we \appl{g}{\R}{\R}$ una función diferenciable y estrictamente creciente.
	\[\implies f_Y(y)=f_X\big(g^{-1}(y)\big) \frac{1}{g'\big(g^{-1}(y)\big)}\]
	De manera similar, si $g$ es estrictamente decreciente$\ds\implies f_Y(y)= - f_X\big(g^{-1}(y)\big) \frac{1}{g'\big(g^{-1}(y)\big)}$
	\begin{dem}
		\[\begin{aligned}
				F_Y(y) & = P(Y\leq y) = P(g(X)\leq y) = P(X\leq g^{-1}(y)) = F_X\big(g^{-1}(y)\big)                                                                                  \\
				f_Y(y) & = \odv{}{y} F_Y(y) = \odv{}{y} F_X\big(g^{-1}(y)\big) = f_X\big(g^{-1}(y)\big) \odv{}{y} g^{-1}(y) = f_X\big(g^{-1}(y)\big) \frac{1}{g'\big(g^{-1}(y)\big)}
			\end{aligned}\]
		En el caso de $g$ decreciente tenemos:
		\[\begin{aligned}
				F_Y(y) & = P(g(X)\leq y) = P(X\geq g^{-1}(y)) = 1 - P(X\leq g^{-1}(y)) = 1 - F_X\big(g^{-1}(y)\big)                                                  \\
				f_Y(y) & = -\odv{}{y} F_X\big(g^{-1}(y)\big) = -f_X\big(g^{-1}(y)\big) \odv{}{y} g^{-1}(y) = -f_X\big(g^{-1}(y)\big) \frac{1}{g'\big(g^{-1}(y)\big)}
			\end{aligned}\]
	\end{dem}
\end{teo}

\fecha{20/03/2024}
\begin{ejer}
	Sea $X$ una variable aleatoria con función de densidad $f_X(x)$ y $Y=g(X)$ con $g(x)=x^2$. ¿Cuál es la función de densidad de $Y$?
	\[F_Y(y) = P(X^2\leq y)=\begin{cases}
			y<0 \implies 0 \\
			y\geq 0 \implies P\left(-\sqrt{y}\leq X\leq \sqrt{y}\right) = F_X\left(\sqrt{y}\right)-F_X\left(-\sqrt{y}\right)
		\end{cases}\]
	\[\implies f_Y(y)=\begin{cases}
			y \leq 0 \implies 0 \\
			y > 0 \implies \ds \odv{}{y} F_X\left(\sqrt{y}\right)-\odv{}{y} F_X\left(-\sqrt{y}\right) = \frac{1}{2\sqrt{y}} f_X\left(\sqrt{y}\right) + \frac{1}{2\sqrt{y}} f_X\left(-\sqrt{y}\right)
		\end{cases}\]
\end{ejer}

\begin{ejem}
	Sea $\ds X\sim\normal{\mu, \sigma^2} \implies \forall x \in \R: f_X(x)=\frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$ e $Y=e^X$ (se denomina lognormal). Calculamos la derivada y la inversa de $g(x)=e^x$.
	\[\implies g'(x)=e^x \quad \we \quad  g^{-1} (y) = \ln{y} \quad \we \quad  \odv{}{y} g^{-1} (y) = \frac{1}{y}\]
	Como $g$ es estrictamente creciente, aplicamos el teorema anterior:
	\[f_Y(y) = \begin{cases}
			0,                                                                                 & \text{ si } y\leq 0 \\
			\ds \frac{1}{\sqrt{2\pi}\sigma} \frac{1}{y} e^{-\frac{(\ln{y}-\mu)^2}{2\sigma^2}}, & \text{ si } y>0     %TODO: gráfica
		\end{cases}\]
\end{ejem}

\begin{ejem}
	Sea $X\sim\EXP{\lambda}, \lambda > 0$ y $Y=3X+2$. Calculamos la función de densidad de $Y$.
	\[g'(x) = 3 \quad \we \quad \odv{}{y} g^{-1}(y) = \odv{}{y}\left(\frac{y-2}{3}\right) = \frac{1}{3}\]
	Como $g$ es estrictamente creciente, aplicamos el teorema anterior:
	\[f_Y(y) = f_X\left(\frac{y-2}{3}\right) \cdot \frac{1}{3}= \begin{cases}
			0,                                                 & \text{ si } y<2     \\
			\ds \frac{1}{3}\lambda e^{-\lambda \frac{y-2}{3}}, & \text{ si } y\geq 2
		\end{cases}\]
\end{ejem}

\subsection{Esperanzas de v.a.c.}
\begin{defn}[Esperanza]
	Sea $X$ una v.a.c. con función de densidad $f_X$. $E(X)$ es la esperanza de $X \ds \iff \boxed{E(X) = \int_{-\infty}^{\infty} x \cdot f_X(x) \odif{x}}$

	Siempre que haya convergencia absoluta $\ds \left(\iff \int_{-\infty}^{\infty} \abs{x} \cdot f_X(x) \odif{x} < \infty\right)$.
\end{defn}

\begin{teo}
	Sea $X$ una v.a.c. con función de densidad $f_X$ y $\appl{g}{\R}{\R}$ una función diferenciable y estrictamente creciente $\ds \implies E(g(X)) = \int_{-\infty}^{\infty} g(x) \cdot f_X(x) \odif{x}$
	\begin{dem}
		\[E(g(X)) = \int_{-\infty}^{\infty} y \cdot f_Y(y) \odif{y} = \int_{-\infty}^{\infty} y \cdot f_X\big(g^{-1}(y)\big) \frac{1}{g'\big(g^{-1}(y)\big)} \odif{y}\]
		Por el cambio de variable $y=g(x)\implies g^{-1}(y) = x \we \odif{y} = g'(x) \odif{x}$.
		\[\implies E(g(X)) = \int_{-\infty}^{\infty} g(x) \cdot f_X(x) \frac{1}{g'(x)} \cdot g'(x) \odif{x} = \int_{-\infty}^{\infty} g(x) \cdot f_X(x) \odif{x}\]
	\end{dem}
\end{teo}

\begin{ejem}
	\begin{enumerate}
		\item[]
		\item Sea $\ds X\sim\unif{a,b}$, entonces $\ds f_X(x)=\frac{1}{b-a}\cdot \mathbbm{1}_{[a,b]}(x)$.
		      \[\implies \lbox{E(X)} = \int_{a}^{b} x \cdot \frac{1}{b-a} \odif{x} = \frac{1}{b-a} \left[\frac{x^2}{2}\right]_{a}^{b} = \frac{b^2-a^2}{2(b-a)} = \rbox{\ds \frac{a+b}{2}}\]
		      \[\begin{aligned}
				      \implies \lbox{V(X)} & = E\left(X^2\right) - E(X)^2 = \int_{a}^{b} x^2 \cdot \frac{1}{b-a} \odif{x} - \left(\frac{a+b}{2}\right)^2 = \frac{1}{b-a} \left[\frac{x^3}{3}\right]_{a}^{b} - \left(\frac{a+b}{2}\right)^2 \\
				                           & = \frac{b^3-a^3}{3(b-a)} - \left(\frac{a+b}{2}\right)^2 = \frac{b^2+ab+a^2}{3} - \frac{a^2+2ab+b^2}{4}                                                                                        \\
				                           & = \frac{4b^2-4ab+4a^2-3a^2-6ab-3b^2}{12} = \frac{b^2-2ab+a^2}{12} = \rbox{\ds \frac{(b-a)^2}{12}}
			      \end{aligned}\]
		\item Sea $X\sim\EXP{\lambda}, \lambda > 0$, entonces $\ds f_X(x)=\lambda e^{-\lambda x} \cdot \mathbbm{1}_{[0,\infty)}(x)$.
		      \[\implies \lbox{E(X)}= \int_{0}^{\infty} x \cdot \lambda e^{-\lambda x} \odif{x} = \left[-x e^{-\lambda x}\right]_{0}^{\infty} + \int_{0}^{\infty} e^{-\lambda x} \odif{x} = 0 + \left[-\frac{1}{\lambda} e^{-\lambda x}\right]_{0}^{\infty} = \rbox{\ds \frac{1}{\lambda}}\]
		      \[\implies \lbox{V(X)} = E\left(X^2\right) - E(X)^2 = \int_{0}^{\infty} x^2 \cdot \lambda e^{-\lambda x} \odif{x} - \left(\frac{1}{\lambda}\right)^2 = \rbox{\ds \frac{1}{\lambda^2}} \tex{ porque }\]
		      \[\int_{0}^{\infty} x^2 \cdot \lambda e^{-\lambda x} \odif{x} = \left[-x^2 e^{-\lambda x}\right]_{0}^{\infty} + 2\int_{0}^{\infty} x e^{-\lambda x} \odif{x} = 0 + 2 \cdot\frac{1}{\lambda^2} = \frac{2}{\lambda^2}\]
		\item Sea $X\sim\normal{0, 1}$, entonces $\ds f_X(x)=\frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}$.
		      \[\lbox{E(X)} = \int_{-\infty}^{\infty} x \cdot \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} \odif{x} = \rbox{0} \tex{  (integrando impar en región centrada en el origen).}\]
		      \[\begin{aligned}
				      \lbox{V(X)} & = E(X^2) - E(X)^2 = \int_{-\infty}^{\infty} x^2 \cdot \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} \odif{x} = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} x^2 \cdot e^{-\frac{x^2}{2}} \odif{x}                   \\
				                  & = \frac{1}{\sqrt{2\pi}} \left(\left[-xe^{-\frac{x^2}{2}}\right]_{-\infty}^{\infty} + \int_{-\infty}^{\infty} e^{-\frac{x^2}{2}} \odif{x}\right) = \frac{1}{\sqrt{2\pi}} \left(0 + \sqrt{2\pi}\right) = \rbox{1}
			      \end{aligned}\]
		\item Sea $X\sim\normal{\mu, \sigma^2}$, entonces $\ds f_X(x)=\frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$.
		      \[E(X) = \int_{-\infty}^{\infty} x \cdot \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}} \odif{x} = \frac{1}{\sqrt{2\pi}\sigma} \int_{-\infty}^{\infty} x \cdot e^{-\frac{(x-\mu)^2}{2\sigma^2}} \odif{x}\]
		      Con el cambio de variable $t=\frac{x-\mu}{\sigma} \implies x=\sigma t + \mu \we \odif{x} = \sigma \odif{t}$.
		      \[\begin{aligned}
				      \implies\lbox{E(X)} & = \frac{1}{\sqrt{2\pi}\sigma} \int_{-\infty}^{\infty} (\sigma t + \mu) e^{-\frac{t^2}{2}} \sigma \odif{t} = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} (\sigma t + \mu) e^{-\frac{t^2}{2}} \odif{t}                            \\
				                          & = \frac{1}{\sqrt{2\pi}} \left(\sigma \int_{-\infty}^{\infty} t e^{-\frac{t^2}{2}} \odif{t} + \mu \int_{-\infty}^{\infty} e^{-\frac{t^2}{2}} \odif{t}\right) = \frac{1}{\sqrt{2\pi}} \left(0 + \mu \sqrt{2\pi}\right) = \rbox{\mu}
			      \end{aligned}\]
		      \[\implies V(X) = E\left[\left(X-E(X)\right)^2\right]= \frac{1}{\sqrt{2\pi}\sigma} \int_{-\infty}^{\infty} (x-\mu)^2 e^{-\frac{(x-\mu)^2}{2\sigma^2}} \odif{x}\]
		      Con el cambio de variable $t=\frac{x-\mu}{\sqrt{2}\sigma} \implies x=\sqrt{2}\sigma t + \mu \we \odif{x} = \sqrt{2}\sigma \odif{t}$.
		      \[V(X) = \frac{1}{\sqrt{\cancel{2}\pi}\cancel{\sigma}} \int_{-\infty}^{\infty} \left(\sqrt{2}\sigma t\right)^2 e^{-t^2} \cancel{\sqrt{2}\sigma} \odif{t} = \frac{2\sigma^2}{\sqrt{\pi}} \int_{-\infty}^{\infty} t^2 e^{-t^2} \odif{t} = \frac{4\sigma^2}{\sqrt{\pi}} \int_{0}^{\infty} t^2 e^{-t^2} \odif{t}\]
		      Consideramos $\ds \Gamma(x) \defeq \int_{0}^{\infty} t^{x-1} e^{-t} \odif{t}$ y el cambio de variable $t^2 = u \implies 2t \odif{t} = \odif{u}$.
		      \[\begin{aligned}
				      \implies \lbox{V(X)} & = \frac{4\sigma^2}{\sqrt{\pi}} \int_{0}^{\infty} u e^{-u} \frac{1}{2\sqrt{u}} \odif{u} = \frac{2\sigma^2}{\sqrt{\pi}} \int_{0}^{\infty} u^{\frac{3}{2} - 1}e^{-u} \odif{u} = \frac{2\sigma^2}{\sqrt{\pi}} \Gamma\left(\frac{3}{2}\right) \\
				                           & = \frac{\cancel{2}\sigma^2}{\sqrt{\pi}} \cdot \frac{1}{\cancel{2}} \cdot \Gamma\left(\frac{1}{2}\right) = \frac{\sigma^2}{\cancel{\sqrt{\pi}}} \cdot \cancel{\sqrt{\pi}} = \rbox{\sigma^2}
			      \end{aligned}\]
	\end{enumerate}
\end{ejem}
\fecha{21/03/2024}
\subsubsection{Calculando con la normal \allbold{$\left(\normal{\mu, \sigma^2}\right)$}}
Sea $X\sim\normal{\mu, \sigma^2}$ con parámetros $\mu\in\R$, $\sigma^2>0$\
\[\implies f(x) = \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}} \quad \we \quad F(x) = \int_{-\infty}^{x} f(y) \odif{y}\]
El caso particular de $\normal{0, 1}$ se denota por $\phi(x) \defeq f(x)$ y $\Phi(x)\defeq F(x)$.
\begin{center}
	\begin{tikzpicture}
		\begin{axis}[
				width=10cm, % Set the desired width
				height=4cm, % Set the desired height
				xlabel=$x$,
				ylabel=$f(x)$,
				xmin=-5, xmax=5,
				ymin=0, ymax=0.5,
				axis lines=middle,
				xtick={-5,-4,-3,-2,-1,0,1,2,3,4,5},
				ytick={0,0.1,0.2,0.3,0.4,0.5},
				ticklabel style={font=\tiny},
			]
			\addplot[domain=-5:5, samples=100, color=blue]{1/(sqrt(2*pi))*exp(-(x^2)/(2*1))};
		\end{axis}
	\end{tikzpicture}
\end{center}

\allbold{¡Basta con $\normal{0,1}$!}
\[X\sim\normal{0, 1} \implies Y\defeq\sigma X + \mu \sim \normal{\mu, \sigma^2}\]
\[Y\sim\normal{\mu, \sigma^2} \implies X\defeq \frac{Y-\mu}{\sigma} \sim \normal{0, 1}\]
Podemos tipificar cualquier v.a.c. $X$ con esperanza $E(X)$ y varianza $V(X)$.
\[Y\defeq \frac{X-E(X)}{V(X)} \implies E(Y) = 0 \we V(Y) = 1\]

\textbf{¿Qué cálculos queremos hacer con \allbold{$Y\sim\normal{\mu, \sigma^2}$?}}
\[E(Y) = E(\mu + \sigma X) = \mu + \sigma E(X) = \mu\, \,\we\, \, V(Y) = V(\mu + \sigma X) = \sigma^2 V(X) = \sigma^2\]
\[E\left(Y^7\right) = E\left(\left(\mu + \sigma X\right)^7\right) = \sum_{k=0}^7 \binom{7}{k} \mu^7 \sigma^{7-k} E(X^{7-k})\]
Caso particular de $X\sim \normal{0,1} \implies \ds E(X^k) = \begin{cases}
		0,                                                                                          & \text{ si } k \text{ es impar} \\
		\ds \frac{1}{2^{\sfrac{k}{2}}} \frac{k!}{\left(\sfrac{k}{2}\right)!} = \frac{(k-1)!!}{k!!}, & \text{ si } k \text{ es par}
	\end{cases}$

\textbf{¿Cómo calculamos las probabilidades de \allbold{$X\sim\normal{0,1}$?}}
\[\forall a, b \in \R : P(a\leq X \leq b) = \Phi(b) - \Phi(a) \tex{ (se calcula numéricamente)}\]
\[\forall a \in \R : P\left(\abs{X} \leq a\right) = P(-a \leq X\leq a) = \Phi(a) - \Phi(-a) = 2\Phi(a) - 1\]

\textbf{¿Qué hago si me dan \allbold{$P(X \leq a)$ y me piden $a$}?}
\[\Phi(a) = P(X\leq a) \implies a = \Phi^{-1}(P(X\leq a)) \tex{ que también se hace numericamente.}\]

\begin{obs}
	$ \ds \operatorname{erf} (x) = \frac{2}{\sqrt{2}} \int_{0}^{x} e^{-y^2} \odif{y} \implies \operatorname{erf} (x) = 2 \Phi(\sqrt{2}x) - 1$
\end{obs}

Para $X\sim\normal{\mu, \sigma^2}$, se tiene $\ds P(Y\leq a) = P\left(\mu + \sigma X \leq a\right) = P\left(X\leq \frac{a-\mu}{\sigma}\right) = \Phi\left(\frac{a-\mu}{\sigma}\right)$.

\textbf{Un famoso resultado: El percentil 95}
\[P(\abs{X} \leq a) = \frac{95}{100} \implies 2\Phi(a) - 1 = \frac{95}{100} \implies a = \Phi\left(\frac{1+ 0.95}{2}\right) \approx 1.96\]

\fecha{02/04/2024}
\subsection{Modelos multidimensionales (vectores aleatorios)}

\begin{defn}[Función de distribución conjunta]
	Sean $X$ e $Y$ dos variables aleatorias, $F_{X, Y}$ es su función de distribución conjunta
	\[\iff \forall (x, y) \in \R^2 : F_{X, Y}(x, y) = P(X \leq x \we Y \leq y)\]

\end{defn}

\begin{obs}
	\begin{itemize*}[itemjoin=\hspace{1cm}]
		\item $\ds \lim_{x, y \to \infty} F_{X, Y}(x, y) = 1$
		\item $\ds\forall \hat{x} \leq x, \hat{y} \leq y : F_{X, Y} (\hat{x}, \hat{y}) \leq F_{X, Y} (x, y)$
	\end{itemize*}
\end{obs}

Funciones de densidad marginales: $\ds \forall x \in \R : F_X(x) = P(X \leq x) = \lim_{y \to \infty} F_{X, Y} (x, y)$

¿Independencia de $X$ e $Y$? $\ds X, Y \tex{ independientes} \iff F_{X, Y}(x, y) = F_X(x) \cdot F_Y(y)$

\begin{defn}[Función de densidad conjunta]
	Sean $X$ e $Y$ dos variables aleatorias, $f_{X, Y}$ es su función de densidad conjunta
	\[\iff \forall (x, y) \in \R^2 : F_{X, Y}(x, y) = \int_{-\infty}^{x} \int_{-\infty}^{y} f_{X, Y}(u, v) \odif{u} \odif{v}\]
\end{defn}

\begin{obs}
	\begin{itemize*}[itemjoin=\hspace{1cm}]
		\item $\forall (x, y) \in\R^2 : f_{X, Y}(x, y) \geq 0$
		\item $\ds \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f_{X, Y}(u, v) \odif{u} \odif{v} = 1$
	\end{itemize*}
\end{obs}

Cálculo de probabilidades: $\ds \forall A \subset \R^2 : P((X, Y)\in A) = \iint_{A} f_{X, Y}(x, y) \odif{x} \odif{y}$

Si $F_{X, Y} (x, y)$ es el dato, $\ds f_{X, Y}(x, y) = \begin{cases} \ds \pdv{}{x, y} F_{X, Y}(x, y) &\tex{si la derivada existe} \\ 0 &\tex{si no existe}\end{cases}$

\begin{ejem}
	Sean $a, b \in \R$ y $(X, Y) \sim \unif{\mathcal{R} \defeq [0, a] \times [0, b]}$
	\[\implies f_{X, Y} = \begin{cases}
			0,            & \text{si } (x, y) \notin \mathcal{R} \\
			\frac{1}{ab}, & \text{si } (x, y) \in \mathcal{R}
		\end{cases}\]
	\[\implies \forall A \subset \mathcal{R} : P((x, y) \in A) = \iint_{A} \frac{1}{ab} \odif{x} \odif{y} = \frac{1}{ab} \operatorname{\tex{Área}}(A)\]
\end{ejem}

\fecha{03/04/2024}

\begin{ejem}[Normal bidimensional]
	Sean $X, Y$ dos variables aleatorias con función de densidad conjunta dependiente de cinco parámetros: $\mu_1\in\R$, $\mu_2\in\R$, $\sigma_1>0$, $\sigma_2>0$, $\rho\in (-1, 1)$; $(X, Y)$ siguen una distribución normal bidimensional
	\[\iff f_{X, Y}(x, y) = \frac{1}{2\pi \sigma_1 \sigma_2 \sqrt{1-\rho^2}} e^{-\frac{1}{2(1-\rho^2)}\left(\frac{(x-\mu_1)^2}{\sigma_1^2} - 2\rho\frac{(x-\mu_1)(y-\mu_2)}{\sigma_1\sigma_2} + \frac{(y-\mu_2)^2}{\sigma_2^2}\right)}\]

	Para $\mu_1 = \mu_2 = 0, \sigma_1 = \sigma_2 = 1$ se tiene la normal bidimensional estándar (dependiente de $\rho$)
	\[f_{X, Y}(x, y) = \frac{1}{2\pi \sqrt{1-\rho^2}} e^{-\frac{1}{2(1-\rho^2)}\left(x^2 - 2\rho x y + y^2\right)}\]

	\textbf{Notación matricial}
	\[\vv{\mu} = \begin{pmatrix}
			\mu_1 \\
			\mu_2
		\end{pmatrix} \quad \mathbb{X} = \begin{pmatrix}
			X \\
			Y
		\end{pmatrix} \quad \Sigma \defeq \begin{pmatrix}
			\sigma^2               & \rho \sigma_1 \sigma_2 \\
			\rho \sigma_1 \sigma_2 & \sigma_2^2
		\end{pmatrix} \implies \Sigma^{-1} = \frac{1}{\sigma_1^2\sigma_2^2(1 - \rho^2)} \begin{pmatrix}
			\sigma_2^2              & -\rho \sigma_1 \sigma_2 \\
			-\rho \sigma_1 \sigma_2 & \sigma_1^2
		\end{pmatrix}\]
	\[\implies f_\mathbb{X}(\vec{x}) = \frac{1}{2\pi \sqrt{\abs{\Sigma}}} e^{\ds -\frac{1}{2} (\vec{x} - \vec{\mu})^T \Sigma^{-1} (\vec{x} - \vec{\mu})}\]
\end{ejem}

\subsubsection{Normal multidimensional}
\[\mathbb{X} = \left(X_1, \dots, X_n\right) \quad \vec{\mu} = \left(\mu_1, \dots, \mu_n\right)
	\quad \Sigma \tex{ simétrica definida positiva $n\times n$}\]
\[\implies f_\mathbb{X} (\vec{x})= \frac{1}{(2\pi)^{\sfrac{n}{2}} \sqrt{\abs{\Sigma}}} e^{\ds -\frac{1}{2} (\vec{x} - \vec{\mu})^T \Sigma^{-1} (\vec{x} - \vec{\mu})}\]

\subsubsection{Marginales e independencia}

\[\forall x \in \R : f_{X} (x) = \int_{-\infty}^{\infty} f_{X, Y}(x, y) \odif{y} \quad \we \quad \forall y \in \R :  f_{Y} (y) = \int_{-\infty}^{\infty} f_{X, Y}(x, y) \odif{x}\]

\begin{teo}
	Sean $X, Y$ dos variables aleatorias con función de densidad conjunta $f_{X, Y}$
	\[ X \tex{ e } Y \tex{ independientes} \iff \exists \, h, g : \forall (x, y) \in \R : f_{X, Y}(x, y) = h(x) \cdot g(y)\]
\end{teo}

\begin{ejem}
	\begin{enumerate}
		\item $f(x, y) = \begin{cases}
				      e^{-x-y} & \text{si } x, y \geq 0 \\
				      0        & \text{en otro caso}
			      \end{cases} \implies f(x, y) = e^{-x-y} \cdot \mathbbm{1}_{\{x>0 \we y>0\}}(x, y)$
		      \[\implies f(x, y) = \left(e^{-x} \cdot \mathbbm{1}_{\{x\geq 0\}}(x)\right) \cdot \left(e^{-y} \cdot \mathbbm{1}_{\{y\geq 0\}}(y)\right) \implies \tex{son independientes}\]
		\item $f(x, y) = \begin{cases}
				      2e^{-x-y} & \text{si } 0 < x < y \\
				      0         & \text{en otro caso}
			      \end{cases} \implies f(x, y) = 2e^{-x-y} \cdot \mathbbm{1}_{\{0<x<y\}}(x, y)$

		      Por tanto, no son independientes porque si lo fueran, tendríamos
		      \[P(c < X < d \we b < Y < a) = P(c < X < d) \cdot P(b < Y < a)\]
	\end{enumerate}
\end{ejem}

\fecha{04/04/2024}
\subsection{Condicionando}

Sean $X$ e $Y$ dos variables aleatorias con función de densidad conjunta $f_{X, Y}$ y $f_X(x)$.

En general $\ds \forall A, B \subset \R^2 : P((X, Y) \in A \vert (X, Y) \in B) = \frac{P((X, Y) \in A \we (X, Y) \in B)}{P((X, Y) \in B)}$ con $P((X, Y) \in B) > 0$. Sin embargo a veces la información ``nueva'' $((X, Y)\in B)$ es muy precisa, por ejemplo $X=3$, entonces la fórmula no vale porque $P(X=3) = 0$.

\begin{defn}[Función de densidad condicionada]
	Sean $X, Y$ dos variables aleatorias con función de densidad conjunta $f_{X, Y}$ y funciones de densidad marginales $f_X(x)$ y $f_Y(y)$.
	\begin{itemize}
		\item $f_{Y|X}(y|x) = f_{Y|X=x}(y)$ es la función de densidad de $Y$ condicionada a $X=x$
		      \[\iff f_{Y|X=x} (y)\defeq \frac{f_{X, Y}(x, y)}{f_X(x)} \quad (\tex{allá donde } f_X(x) > 0)\]
		\item $f_{X|Y}(x|y) = f_{X|Y=y}(x)$ es la función de densidad de $X$ condicionada a $Y=y$
		      \[\iff f_{X|Y=y} (x)\defeq \frac{f_{X, Y}(x, y)}{f_Y(y)} \quad (\tex{allá donde } f_Y(y) > 0)\]
	\end{itemize}
	Una comprobación necesaria es que $\ds \int_{-\infty}^{\infty} f_{Y|X=x} (y) \odif{y} = 1$ y $\ds \int_{-\infty}^{\infty} f_{X|Y=y} (x) \odif{x} = 1$:
	\[\int_{-\infty}^{\infty} f_{Y|X=x} (y) \odif{y} = \frac{1}{f_X(x)} \int_{-\infty}^{\infty} f_{X, Y}(x, y) \odif{y} = \frac{1}{f_X(x)} f_X(x) = 1\]
\end{defn}

\begin{ejem}%TODO: dibujo
	$\ds f_{X, Y}(x, y) = \begin{cases}
			2e^{-x}e^{-y} & \text{si } 0 < x < y \\
			0             & \text{en otro caso}
		\end{cases}$
	\[\begin{aligned}
			\implies \forall x > 0 :f_X(x) & = \int_{-\infty}^{\infty} f_{X, Y}(x, y) \odif{y} = \int_{x}^{\infty} 2e^{-x}e^{-y} \odif{y} = 2e^{-x} \int_{x}^{\infty} e^{-y} \odif{y} \\
			                               & = 2e^{-x} e^{-x} = 2e^{-2x}
		\end{aligned}\]
	\[\forall y > 0 : \lbox{f_Y(y)} = \int_{-\infty}^{\infty} f_{X, Y}(x, y) \odif{x} = \int_{0}^{y} 2e^{-x}e^{-y} \odif{x} = 2e^{-y} \int_{0}^{y} e^{-x} \odif{x} = \rbox{2e^{-y} (1 - e^{-y})}\]
	\[f_{Y|X=3} (y) = \frac{f_{X, Y}(3, y)}{f_X(3)} = \frac{2e^{-3}e^{-y}}{2e^{-6}} = e^{3-y} \cdot \mathbbm{1}_{\{y>3\}}(y)\]
	\[\implies f_{X|Y=y} (x) = \frac{f_{X, Y}(x, y)}{f_Y(y)} = \frac{2e^{-x}e^{-y}}{2e^{-y}(1-e^{-y})} = \frac{e^{-x}}{1-e^{-y}} \cdot \mathbbm{1}_{\{0<x<y\}}(x)\]
	¡Cuidado! No se nos puede olvidar el soporte. \hspace{\fill} \textbf{(Motivo de excomunión)}
\end{ejem}

\textbf{Nota sobre independencia:} $X, Y \tex{ indep.} \implies f_{X|Y=y}(x) = f_X(x) \we f_{Y|X=x}(y) = f_Y(y)$

\subsection{Transformaciones / cambio de variables}

$\begin{aligned}
		\tex{Sean }X, Y \tex{ dos v.a. con función de densidad conjunta }f_{X, Y} \we T\colon \R^2 & \longrightarrow  \R^2          \\[-0.6em]
		(x, y)                                                                                     & \longmapsto (u(x, y), v(x, y))
	\end{aligned}$\\
Definimos las variables aleatorias $U \defeq u(X, Y)$ y $V \defeq v(X, Y)$.

Por ejemplo, si tenemos $T$ lineal con matriz asociada $\begin{pmatrix}
		a & b \\
		c & d
	\end{pmatrix} \implies \begin{cases}
		U = aX + cY \\
		V = bX + dY
	\end{cases}$ \\pero también hay transformaciones no lineales como $U = X + Y \we V = \frac{X}{X+Y}$.

Definimos $D\defeq \{(x, y) \in \R : f_{X, Y}(x, y) > 0\}$ y sea $B \subset T(D)$.
\[\begin{aligned}
		\implies P((U, V) \in B) & = \iint_B f_{U, V}(u, v) \odif{u} \odif{v} = \iint_{T^{-1}(B)} f_{X, Y}(x, y) \odif{x} \odif{y} \\
		                         & = \iint_{B} f_{X, Y}(x(u, v), y(u, v)) \abs{\operatorname{J}(u, v)} \odif{u} \odif{v}
	\end{aligned}\]

\begin{teo}
	Sean $X, Y$ dos variables aleatorias con función de densidad conjunta $f_{X, Y}$ y sea $\appl{T}{\R^2}{\R^2} : (x, y) \mapsto T(x, y) = (u(x, y), v(x, y))$ una biyección de $D$ en $T(D)$ de clase $C^1$ con inversa de clase $C^1$.
	\[\implies f_{U, V}(u, v) = \begin{cases}
			f_{X, Y}(x(u, v), y(u, v)) \abs{\operatorname{J}(u, v)} & \text{si } (u, v) \in T(D) \\
			0                                                       & \text{en otro caso}
		\end{cases}\]
\end{teo}

\fecha{09/04/2024}

\begin{ejem}
	Sean $X, Y$ v.a. con $\ds \forall x, y > 0 : f_{X, Y}(x, y) \defeq \frac{1}{4} e^{-\frac{1}{2}(x+y)}$.\\
	Sea $T(x, y) = \left(\frac{x-2}{2}, y\right)$. Definimos $(U, V) \defeq T(X, Y)$.
	\[\implies \forall x, y > 0 : T(x, y) = \begin{pmatrix} x & y\end{pmatrix}\begin{pmatrix}
			\frac{1}{2} & 0 \\
			0           & 1
		\end{pmatrix} - \begin{pmatrix} \frac{1}{2} \\ 0 \end{pmatrix} \implies \begin{cases}
			x = 2u + 2 \\
			y = v
		\end{cases}\]
	\[\implies T^{-1} (u, v) = (2u+v, v) \we \operatorname{J}(u, v) = \begin{vmatrix}
			\pdv{x}{u} & \pdv{x}{v} \\
			\pdv{y}{u} & \pdv{y}{v}
		\end{vmatrix}=\begin{vmatrix}
			2 & 1 \\
			0 & 1
		\end{vmatrix} = 2\]
	\[\implies f_{U, V}(u, v) = \begin{cases}
			2\frac{1}{4}e^{-\frac{1}{2}[2u+v+v]} = \frac{1}{2} e^{-(u+v)} & \text{si } v > 0 \we u > -\frac{v}{2} \\
			0                                                             & \text{en otro caso}
		\end{cases}\]
\end{ejem}

\begin{ejem}
	Sean $X, Y$ v.a. con $\ds \forall x, y > 0 : f_{X, Y}(x, y) \defeq e^{-x-y}$.\\
	Sea $\ds T(x, y) = \left(x+y, \frac{x}{x+y}\right)$. Definimos $(U, V) \defeq T(X, Y)$.
	\[\implies T^{-1}(u, v) = (u\cdot v, u(1-v)) \we \operatorname{J}(u, v) = \begin{vmatrix}
			\pdv{x}{u} & \pdv{x}{v} \\
			\pdv{y}{u} & \pdv{y}{v}
		\end{vmatrix} = \begin{vmatrix}
			v   & u  \\
			1-v & -u
		\end{vmatrix} = -u\]
	\[\implies f_{U, V}(u, v) = \begin{cases}
			\abs{-u} e^{-u\cdot v - u(1-v)} = u e^{-u} & \text{si } u > 0 \we 0 < v < 1 \\
			0                                          & \text{en otro caso}
		\end{cases}\]
	\[\implies \forall v \in (0,1) : f_V(v) = \int_{-\infty}^{\infty} f_{U, V}(u, v) \odif{u} = \int_{0}^{\infty} u e^{-u} \odif{u} = \left[-e^{-u}(u+1)\right]_{0}^{\infty} = 1\]
	\[\implies V\sim\unif{0,1}\]
\end{ejem}

\fecha{10/04/2024}

Sean $X, Y$ dos variables con función de densidad conjunta $f_{X, Y}$, consideramos una función $\appl{g}{\R^2}{\R}$ ``razonable''$^{(*)}$ y definimos $Z \defeq g(X, Y)$. Queremos calcular $E(Z), V(Z)$.

\hfill $(*)$ $g$ cumple que $\ds \forall z \in \R : \{\omega \in \Omega : g(X(\omega), Y(\omega)) \leq z\} \in \F$
\begin{teo}
	Sean $X, Y$ dos variables aleatorias con $f_{X, Y}$ y $Z \defeq g(X, Y)$ con $\appl{g}{\R^2}{\R}$
	\[\implies E(Z)= \int_{-\infty}^{\infty} z \cdot f_Z(z) \odif{z} = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x, y) \cdot f_{X, Y} (x, y) \odif{x} \odif{y}\]
\end{teo}
\begin{obs}
	\begin{enumerate}
		\item $\ds \forall \alpha, \beta \in \R : E(\alpha X + \beta Y) = \alpha E(X) + \beta E(Y)$
		\item $\ds E(X\cdot Y) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x\cdot y \cdot f_{X, Y}(x, y) \odif{x} \odif{y}$
		      \[\cov{X, Y} = E(X\cdot Y) - E(X)E(Y) \quad\we\quad \rho(X, Y) = \frac{\cov{X, Y}}{\sqrt{V(X)V(Y)}}\]
		      \[\implies X, Y \tex{ indep} \stackrel{\nimpliedby}{\implies} \rho(X, Y) = 0\]
		\item $X, Y$ indep. $\iff f_{X, Y}$ se factoriza
		      \[\implies X, Y \tex{ indep.}\iff \forall \appl{g, h}{\R}{\R} : E(g(X)h(Y)) = E(g(X))E(h(Y))\]
		\item Esperanza condicionada y total
		      \[\begin{aligned}
				      E(Y \vert X=x) & = \int_{-\infty}^{\infty} y\cdot f_{Y\vert X=x} (y) \odif{y} = \int_{-\infty}^{\infty} y\cdot \frac{f_{X, Y}(x, y)}{f_X(x)} \odif{y} \\
				      \implies E(Y)  & = \int_{-\infty}^{\infty} E(Y \vert X=x) \cdot f_X(x) \odif{x}
			      \end{aligned}\]
	\end{enumerate}
\end{obs}
\allbold{¿Y qué hay de $f_Z(z)$?}
Definimos $A_{g}(z) \defeq \{(x, y) \in \R^2 : g(x, y) \leq z\}$
\[\forall z \in \R : F_Z(z)= P(Z \leq z) = P(g(X, Y) \leq z) = P((X, Y) \in A_g(z)) = \iint_{A_{g}(z)} f_{X, Y}(x, y) \odif{x} \odif{y}\]
\[\implies \forall z \in \R : f_Z(z) = \begin{cases}
		\pdv{}{z} F_Z(z) & \text{si la derivada existe} \\
		0                & \text{en otro caso}
	\end{cases}\]
\begin{ejem}[El caso de la suma]
	Sean $X, Y$ dos v.a. con $f_{X, Y}$ y $Z \defeq X + Y$. Entonces $A_+(z) \defeq \{(x, y) \in \R^2 : x+y \leq z\}$.
	\[\implies \forall z \in \R : F_Z(z) = \iint_{A_+(z)} f_{X, Y}(x, y) \odif{x} \odif{y} = \int_{-\infty}^{\infty} \int_{-\infty}^{z-x} f_{X, Y}(x, y) \odif{y} \odif{x}\]
	Mediante el cambio de variables $u = x \we v = x+y$ se tiene
	\[T(x, y) = (x, x+y) \implies T^{-1}(u, v) = (u, v-u) \implies \operatorname{J}(u, v) = \begin{vmatrix} 1 & 0 \\ -1 & 1\end{vmatrix} = 1\]
	\[\implies F_Z(z) = \int_{-\infty}^{z} \int_{-\infty}^{\infty} f_{X, Y}(u, v-u) \odif{u} \odif{v} \implies \lbox{f_Z(z)} = \odv{}{z} F_Z(z) = \rbox{\ds \int_{-\infty}^{\infty} f_{X, Y}(u, z-u) \odif{u}}\]
	\allbold{Caso particular:} Si $X, Y$ independientes$\implies f_{X, Y}(x, y) = f_X(x) \cdot f_Y(y)$
	\[\implies \forall z \in \R : \lbox{\ds f_{X+Y}(z)} = \int_{-\infty}^{\infty} f_X(u) \cdot f_Y(z-u) \odif{u} \eqdef \rbox{\ds (f_X * f_Y)(z)}\]
\end{ejem}

\fecha{11/04/2024}

\subsection{Convolución}

Sean $X$ e $Y$ dos v.a.d. indep. que toman valores en $\Z_{\geq 0}$. Sea $Z = X + Y$.
\[\implies P(Z = k) = \sum_{j=0}^{k} P(X = j \we Y = k-j) = \sum_{j=0}^{k} P(X = j) \cdot P(Y = k-j)\]
Ahora, sean $X$ e $Y$ dos v.a.c. indep. con funciones de densidad $f_X$ y $f_Y$ respectivamente.
\[\implies \int_{-\infty}^{\infty} f_{X}(x) \cdot f_{Y}(z-x) \odif{x} = \left(f_{X} * f_{Y}\right) (z)\] %TODO: Dibujo

\begin{ejem}
	Sean $X, Y \sim \normal{0, 1}$ independientes. Queremos $f_Z$ con $Z \defeq X + Y$.
	\[\begin{aligned}
			\forall z \in \R : f_Z(z) & = \int_{-\infty}^{\infty} f_X(u) \cdot f_Y(z-u) \odif{u} = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{u^2}{2}} \cdot \frac{1}{\sqrt{2\pi}} e^{-\frac{(z-u)^2}{2}} \odif{u}                                                                                         \\
			                          & = \frac{1}{2\pi} \int_{-\infty}^{\infty} e^{-\frac{1}{2}\left[z^2 + 2u^2 - 2uz\right]} \odif{u} \stackrel{(*)}{=} \frac{1}{\sqrt{2\pi}} e^{-\frac{z^2}{4}} \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-\frac{1}{2}\left(\sqrt{2}u - \frac{z}{\sqrt{2}}\right)^2} \odif{u} \\
		\end{aligned}\]
	$(*)$ porque $-\frac{1}{2}(z^2 + 2u^2 - 2uz) = -\frac{1}{2}\left(2u^2 - 2uz +\frac{z^2}{2} + \frac{z^2}{2}\right) = -\frac{1}{2}\left[\left(\sqrt{2}u - \frac{z}{\sqrt{2}}\right)^2 + \frac{z^2}{2}\right]$.

	Por el cambio de variables $w = \sqrt{2} u - \frac{z}{\sqrt{2}} \implies \odif{w} = \sqrt{2} \odif{u} $, se tiene
	\[\forall z \in \R : f_Z(z) = \frac{1}{\sqrt{2}} \frac{1}{\sqrt{2\pi}} e^{-\frac{z^2}{4}} \cancelto{1}{\frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-\frac{w^2}{2}} \odif{w}} = \frac{1}{2\sqrt{\pi}} e^{-\frac{z^2}{4}} \implies Z \sim \normal{0, \left(\sqrt{2}\right)^2}\]
	\textbf{Moraleja:} la suma de normales independientes es una normal.
\end{ejem}

% 	\begin{ejer} Sean $\forall i \in \N_n : X_i \sim \normal{0, 1}$ indep.$\implies \ds Z_n \defeq \sum_{i=1}^{n} X_i \sim \normal{0, n}$.
% 		\begin{dem}
% 			Por inducción suponemos que $\ds Z_{n-1} \sim \normal{0, n-1}$.
% 			\[\begin{aligned}
% 					\forall z \in \R : f_{Z_n}(z) & = \left(f_{Z_{n-1}} * f_{X_1}\right)(z) = \int_{-\infty}^{\infty} f_{Z_{n-1}}(u) \cdot f_{X_1}(z-u) \odif{u}                                                    \\
% 					                              & \stackrel{\tex{HI}}{=} \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}(n-1)} e^{-\frac{u^2}{2(n-1)^2}} \cdot \frac{1}{\sqrt{2\pi}} e^{-\frac{(z-u)^2}{2}} \odif{u} \\
% 					                              & = \frac{1}{\sqrt{2\pi}(n-1)} \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-\frac{1}{2}\left[\frac{u^2}{(n-1)^2} + \left(z-u\right)^2\right]} \odif{u}       \\
% 				\end{aligned}\]
% 			Completamos cuadrados: $\ds -\frac{1}{2}\left[\frac{u^2}{(n-1)^2} + \left(z-u\right)^2\right] = -\frac{1}{2}\left[\frac{u^2}{(n-1)^2} + z^2 - 2uz + u^2\right]$
% 			\[= -\frac{1}{2}\left[u^2\left(\frac{1 + n^2 -2n +1}{(n-1)^2}\right) - 2uz + \frac{z^2}{n} + \frac{n-1}{n}z^2\right] = -\frac{1}{2} \left(u^2\frac{n}{n-1} -2uz + \frac{n-1}{n}z^2\right) - \frac{z^2}{2n}\]
% 			\[\implies -\frac{1}{2}\left[\frac{u^2}{n-1} + \left(z-u\right)^2\right] = -\frac{1}{2} {\left(u\sqrt{\frac{n}{n-1}} - z \sqrt{\frac{n-1}{n}}\right)}^2 - \frac{z^2}{2n}\]
% 			\[\implies \forall z \in \R : f_{Z_n}(z) = \frac{1}{\sqrt{2\pi} (n-1)} e^{-\frac{z^2}{2n}} \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-\frac{1}{2} {\left(u\sqrt{\frac{n}{n-1}} - z \sqrt{\frac{n-1}{n}}\right)}^2} \odif{u}\]
% 			Por el cambio de variables $\ds w = u\sqrt{\frac{n}{n-1}} - z \sqrt{\frac{n-1}{n}} \implies \odif{w} = \sqrt{\frac{n}{n-1}} \odif{u}$, se tiene
% 			\[\begin{aligned}
% 					f_{Z_n}(z) & = \frac{1}{\sqrt{2\pi} (n-1)} e^{-\frac{z^2}{2n}} \frac{1}{\sqrt{2\pi}} \sqrt{\frac{n-1}{n}} \int_{-\infty}^{\infty} e^{-\frac{1}{2} w^2} \odif{w} =
% 				\end{aligned}\]
% 		\end{dem}
% 	\end{ejer}

\begin{ejem}[Normal bidimensional]
	Sean $X, Y$ dos variables aleatorias con función de densidad conjunta $\ds \forall x, y \in \R : f_{X, Y}(x, y) = \frac{1}{2\pi \sqrt{1-\rho^2}} e^{-\frac{1}{2(1-\rho^2)}\left(x^2 - 2\rho x y + y^2\right)}$.
	\[\implies f_X(x) = \int_{-\infty}^{\infty} f_{X, Y}(x, y) \odif{y} = \int_{-\infty}^{\infty} \frac{1}{2\pi \sqrt{1-\rho^2}} e^{-\frac{1}{2(1-\rho^2)}\left(x^2 - 2\rho x y + y^2\right)} \odif{y}\]
	Por tanto, $\ds f_X(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}$ y $\ds f_Y(y) = \frac{1}{\sqrt{2\pi}} e^{-\frac{y^2}{2}}$ (ambas normales estándar).

	Ahora veamos que $\ds \rho(X, Y) \defeq \frac{\cov{X, Y}}{\sqrt{V(X) V(Y)}} = \rho$.
	\[E(X\cdot Y) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x\cdot y \cdot \frac{1}{2\pi \sqrt{1-\rho^2}} e^{-\frac{1}{2(1-\rho^2)}\left(x^2 - 2\rho x y + y^2\right)} \odif{x} \odif{y}\] %TODO: Hacer desarrollo y ver que sale \rho
	Usando esperanza total, $\ds E(g(X, Y)) = \int_{-\infty}^{\infty} E(g(X, Y) \vert X = x) \cdot f_X(x) \odif{x}$
	\[E(X\cdot Y) = \int_{-\infty}^{\infty} E(X\cdot Y \vert X = x) \cdot f_X(x) \odif{x} = \int_{-\infty}^{\infty} x \cdot E(Y \vert X = x) \cdot f_X(x) \odif{x}\]
	Necesitamos calcular $\ds E(Y \vert X = x) = \int_{-\infty}^{\infty} y \cdot f_{Y|X=x}(y) \odif{y}$.
	\[f_{Y|X=x}(y) = \frac{f_{X, Y}(x, y)}{f_X(x)} = \frac{\frac{1}{2\pi \sqrt{1-\rho^2}} e^{-\frac{1}{2(1-\rho^2)}\left(x^2 - 2\rho x y + y^2\right)}}{\frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}} = \frac{1}{\sqrt{2\pi(1-\rho^2)}} e^{-\frac{1}{2(1-\rho^2)}\left(y-\rho x\right)^2}\]
	Es decir, $Y \vert X = x \sim \normal{\rho x, 1-\rho^2}$.
	\[\implies E(Y \vert X = x) = \int_{-\infty}^{\infty} y \cdot \frac{1}{\sqrt{2\pi(1-\rho^2)}} e^{-\frac{1}{2(1-\rho^2)}\left(y-\rho x\right)^2} \odif{y} = \rho x\]
	\[\implies \lbox{E(X\cdot Y)} = \int_{-\infty}^{\infty} x \cdot \rho x \cdot \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} \odif{x} = \rho \underbrace{\int_{-\infty}^{\infty} x^2 f_X(x) \odif{x}}_{E\left(X^2\right)} = \rbox{\rho}\]
\end{ejem}

\fecha{17/04/2024}

\subsection{Fuera de menú}

Tenemos $X_1, X_2$ siguiendo una distribución normal bidimensional con $\mu_1, \mu_2, \sigma_1, \sigma_2, \rho$ conocidos.
\[\implies f_{X, Y}(x,y) = \frac{1}{2\pi \sigma_1 \sigma_2 \sqrt{1-\rho^2}} e^{-\frac{1}{2(1-\rho^2)}\left(\frac{(x-\mu_1)^2}{\sigma_1^2} - 2\rho \frac{(x-\mu_1)(y-\mu_2)}{\sigma_1 \sigma_2} + \frac{(y-\mu_2)^2}{\sigma_2^2}\right)}\]
Podemos escribir $X_1$ y $X_2$ como transformaciones de $Z_1, Z_2 \sim \normal{0, 1}$.
\[\begin{aligned}
		\begin{pmatrix}
			X_1 \\
			X_2
		\end{pmatrix} & = \begin{pmatrix}
			                  \mu_1 \\
			                  \mu_2
		                  \end{pmatrix} + \begin{pmatrix}
			                                  \sigma_1      & 0                        \\
			                                  \rho \sigma_2 & \sigma_2 \sqrt{1-\rho^2}
		                                  \end{pmatrix} \begin{pmatrix}
			                                                Z_1 \\
			                                                Z_2
		                                                \end{pmatrix} = \begin{pmatrix}
			                                                                \mu_1 \\
			                                                                \mu_2
		                                                                \end{pmatrix} + \begin{pmatrix}
			                                                                                \sigma_1 Z_1 \\
			                                                                                \sigma_2 (\rho Z_1 + \sqrt{1-\rho^2} Z_2)
		                                                                                \end{pmatrix}
	\end{aligned}\]
Esta transformación facilita muchos cálculos como $E(X_1 \cdot X_2)$.

Tenemos $\mathbb{X}^T = (X_1, \dots, X_n)$ normal $n$-dimensional con $\vec{\mu} = (\mu_1, \dots, \mu_n)$ y $\mathbb{V}$ matriz de covarianzas definida positiva.
\[\implies f_{\mathbb{X}}(\vec{x}) = \frac{1}{(2\pi)^{n/2} \sqrt{\det{\mathbb{V}}}} e^{-\frac{1}{2} (\vec{x} - \vec{\mu})^T \mathbb{V}^{-1} (\vec{x} - \vec{\mu})}\]
Si ahora tenemos en cuenta que $A$ definida positiva $\iff \exists R$ tal que $A = R^T R$ con $B$ no singular, podemos escribir $\mathbb{V} = U^T U$.

\begin{teo}[de representación]
	\[\mathbbm{X} \sim \normal{\vec{\mu}, \mathbb{V}} \iff \mathbb{X} = \vec{\mu} + U\cdot \Z \tex{ con } \Z \sim \normal{\vec{0}, I}\]
\end{teo}
