\fecha{22/04/2024}

\section{Convergencia de variables aleatorias}

Sea $\ds \left(X_n\right)_{n \in \N}$ una sucesión de variables aleatorias en un espacio de probabilidad $(\Omega, \F, P)$, queremos estudiar las series
\[\big(S_n\big)_{n\in \N}  \defeq\left(\sum_{i=1}^{n} X_i\right)_{n \in \N} \quad \we \quad \big(Z_n\big)_{n\in \N} \defeq \left(\frac{1}{n}\sum_{i=1}^{n} X_i\right)_{n \in \N}\]
Interesan, específicamente, los límites cuando $n\to \infty$ de estas series.
\begin{enumerate}
	\item Sabemos que significa $\ds \lim_{n\to\infty} E(S_n)$ y $\ds \lim_{n\to\infty} V(S_n)$ pero, ¿qué significa $\ds \lim_{n\to\infty} S_n = S$? \\
	      Requerimos de técnicas más avanzadas que se denominan \textbf{modos de convergencia}.
	\item Asumiremos que $\{X_n\}$ son \textbf{independientes e idénticamente distribuidas (i.i.d.)}.
	      \begin{itemize}
		      \item Todas las $X_i$ tienen la misma distribución. Por tanto, es habitual definir una \allbold{$X$ de referencia} que tenga la misma distribución que todas las $X_i$.
		      \item Las $X_i$ son \allbold{completamente independientes}, es decir, para todo subconjunto finito $I \subset \N$, $\{X_i\}_{i\in I}$ son independientes.
	      \end{itemize}
	\item Descubriremos que, para $n$ grande y $X$ de referencia, $Z_n$ se comporta como $E(X)$. Además, veremos el teorema central del límite, que nos dice que $\ds \frac{Z_n-E(X)}{\sfrac{\sigma(Z_n)}{\sqrt{n}}} \approx \normal{0,1}$.
\end{enumerate}
\subsection{Medias y varianzas de las sumas y las medias}
Las medias de $S_n$ y $Z_n$ resultan sencillas de calcular.
\[\begin{aligned}
		E(S_n) & = E\left(\sum_{i=1}^{n} X_i\right) = \sum_{i=1}^{n} E(X_i) \stackrel{(\star)}{=} nE(X)                      \\
		E(Z_n) & = E\left(\frac{1}{n}\sum_{i=1}^{n} X_i\right) = \frac{1}{n}\sum_{i=1}^{n} E(X_i) \stackrel{(\star)}{=} E(X)
	\end{aligned}\]
\hspace*{\fill} $(\star)$ Si $X_i$ tienen la misma media.

Nótese que, en la mayoría de casos, $E(S_n) \xrightarrow{n\to\infty} \infty$ mientras que $E(Z_n)$ tenderá a al promedio de las medias de las $X_i$.

Las varianzas de $S_n$ y $Z_n$ son más intricadas de calcular y debemos tener en cuenta, no solo el caso de que las $X_i$ tengan la misma varianza, sino también si son incorreladas.

\[\begin{aligned}
		V(S_n) & = V\left(\sum_{i=1}^{n} X_i\right) = V(X_1) + V\left(\sum_{i=2}^{n} X_i\right) + 2 \cov{X_1, \sum_{i=2}^{n} X_i}                                                                                                             \\
		       & = V(X_1) + 2 \sum_{i=2}^{n} \cov{X_1, X_i} + V\left(\sum_{i=2}^{n} X_i\right)                                                                                                                                                \\
		       & = V(X_1) + 2 \sum_{i=2}^{n} \cov{X_1, X_i} + V(X_2) + 2 \sum_{i=3}^{n} \cov{X_2, X_i} + V\left(\sum_{i=3}^{n} X_i\right)                                                                                                     \\
		       & = \sum_{i=1}^{n} V(X_i) + 2 \sum_{1\leq i < j \leq n} \cov{X_i,X_j} \stackrel{(*)_1}{=} \sum_{i=1}^{n} V(X_i) \stackrel{(*)_2}{=} n(\sigma(X))^2                                                                             \\
		V(Z_n) & = V\left(\frac{1}{n}\sum_{i=1}^{n} X_i\right) = \frac{1}{n^2} V\left(\sum_{i=1}^{n} X_i\right) \stackrel{(*)_1}{=} \frac{1}{n^2}\sum_{i=1}^{n} V(X_i) \stackrel{(*)_2}{=} \frac{(\sigma(X))^2}{n} \xrightarrow{n\to\infty} 0
	\end{aligned}\]
\hspace*{\fill} $(*)_1$ Si $X_i$ incorreladas, $(*)_2$ Si (además) $X_i$ tienen la misma varianza.

De forma similar a como ocurre con las medias, (en la mayoría de casos) la varianza de $S_n$ tiende a infinito mientras que la de $Z_n$ tiende a 0 cuando $n\to\infty$.

\subsection{Convergencia cuadrática}

\begin{defn}[Convergencia cuadrática]
	Sea $\left(X_n\right)_{n\in \N}$ una sucesión de v.a. y $X$ v.a. en el mismo espacio de probabilidad $(\Omega, \F, P)$, $\ds \boxed{X_n \xrightarrow{\tex{cuad.}} X \iff E\left(\abs{X_n - X}^2\right) \xrightarrow{n\to\infty} 0}$
\end{defn}

\begin{teo}
	Sean $\left(X_n\right)_{n\in \N}$ v.a. i.i.d. con $X$ v.a. de referencia $\ds \implies Z_n\xrightarrow{\tex{cuad.}} E(X)$
	\begin{dem}
		Si denominamos $\mu = E(X) \we V(X) = \sigma^2$, entonces
		\[E\left(\abs{Z_n - \mu}^2\right) = E\left(\abs{Z_n - E(Z_n)}^2\right) = V(Z_n) = \frac{\sigma^2}{n} \xrightarrow{n\to\infty} 0\]
	\end{dem}
\end{teo}

Basta con que sean incorreladas y tengan la misma media y varianza porque es lo único que se ha usado en la demostración. Podemos incluso ofrecer un teorema más general que no requiera que tengan la misma media y varianza.

\begin{teo}
	Sean $\left(X_n\right)_{n\in \N}$ v.a. en $(\Omega, \F, P)$ con $E(X_i) = \mu_i$ y $V(X_i) = \sigma_i^2$. Suponemos que $\ds \mu_n \defeq \frac{1}{n}\sum_{i=1}^{n} \mu_i \xrightarrow{n\to\infty} \mu \in \R$ y $\ds V(Z_n) = \frac{1}{n^2}\sum_{i=1}^{\infty} \sigma_i^2 \to 0 \implies Z_n \xrightarrow{\tex{cuad.}} \mu$
	\begin{dem} (Buen ejercicio de examen)%TODO: terminar demostración
		\[\begin{aligned}
				E\left(\abs{Z_n - \mu}^2\right) & = E\left(\abs{Z_n - \mu_n + \mu_n - \mu}^2\right)                                                \\
				                                & = E\left(\abs{Z_n - \mu_n}^2 + \abs{\mu_n - \mu}^2 + 2(Z_n - \mu_n)(\mu_n - \mu)\right)          \\
				                                & = E\left((Z_n - \mu_n)^2\right) + {(\mu_n - \mu)^2} + 2(\mu_n - \mu)\cancelto{0}{E(Z_n - \mu_n)} \\
				                                & = E\left((Z_n - \mu_n)^2\right) + (\mu_n - \mu)^2 \xrightarrow{n\to\infty} 0
			\end{aligned}\]
	\end{dem}
\end{teo}

Es decir, si el promedio de las medias tiende a un número finito y la varianza del promedio tiende a 0, entonces el promedio tiende al límite del promedio de las medias.

\fecha{23/04/2024}

\subsection{Convergencia en probabilidad (ley débil)}

J.Bernoulli (1713) Ars Conjectandi: Si tienes un dado regular, cuantas más veces lo lances, más se aproximará la frecuencia relativa de un número a su probabilidad.

\begin{defn}[Convergencia en probabilidad]
	Sea $\left(X_n\right)_{n\in \N}$ una sucesión de v.a. y $X$ v.a. en el mismo espacio de probabilidad $(\Omega, \F, P)$,
	\[ X_n \xlongrightarrow{P} X \iff \forall \varepsilon > 0 : \lim_{n\to\infty} P\left(\abs{X_n - X} > \varepsilon\right) = 0\]
\end{defn} %TODO: Añadir dibujo

\begin{teo}[Ley débil de los grandes números]
	Sean $\left(X_n\right)_{n\in \N}$ v.a. i.i.d. en $(\Omega, \F, P)$ con $X$ v.a. de referencia tal que $\mu \defeq E(X) < \infty \we \sigma^2 \defeq V(X) < \infty \ds \implies Z_n \defeq \frac{1}{n} \sum_{j=1}^{n} X_j \xlongrightarrow{P} \mu$
	\begin{dem}
		Dado $\varepsilon > 0$, por la desigualdad de Chebyshev
		\[P\left(\abs{Z_n - \mu} > \varepsilon \right) \leq \frac{V(Z_n)}{\varepsilon^2} = \frac{\sigma^2}{n\varepsilon^2} \xrightarrow{n\to\infty} 0\]
	\end{dem}
\end{teo}

\begin{obs}
	\begin{enumerate}
		\item Hay una ley fuerte de los grandes números.
		\item Hipótesis: No hace falta que sean independientes, basta con que sean incorreladas. Tampoco hace falta que sean idénticas, basta con que tengan la misma media y varianza.
		\item Se podría incluso hacer una variante del teorema para variables aleatorias que no tengan la misma media y varianza.
		\item Existe la posibilidad de adaptar el teorema para que no haga falta que sean incorreladas, solo que tengan una correlación ``pequeña''.
	\end{enumerate}
\end{obs}

\begin{teo}
	Sean $\left(X_n\right)_{n\in \N}$ v.a. incorreladas en $(\Omega, \F, P)$ con $E(X_i) = \mu_i$ y $V(X_i) = \sigma_i^2$. Suponemos $\ds V(Z_n) = \frac{1}{n^2}\sum_{i=1}^{n} \sigma_i^2 \xrightarrow{n\to\infty} 0 \implies Z_n \xlongrightarrow{P} \frac{1}{n} \sum_{i=1}^{n} \mu_i$
	\begin{dem} Por la desigualdad de Chebyshev
		\[P\left(\abs{Z_n - \frac{1}{n} \sum_{i=1}^{n} \mu_i} > \varepsilon \right) \leq \frac{V\left(Z_n\right)}{\varepsilon^2} = \frac{1}{n^2\varepsilon^2}\sum_{i=1}^{\infty} \sigma_i^2 \xrightarrow{n\to\infty} 0\]
	\end{dem}
\end{teo}

Es decir, si la varianza del promedio tiende a 0, entonces el promedio tiende al promedio de las medias.

\allbold{Usos en estadística:} Sean $X_1, \ldots, X_n$ v.a. i.i.d. con $X\sim \ber{p}$ de referencia. Queremos estimar $p$ (desconocido), ¿cuánto de grande tiene que ser $n$ para estar razonablemente seguros que el estimador $\ds \Bar{X}_n \defeq \frac{1}{n}\sum_{i=1}^{n} X_i$ está cerca de $p$?
\[P\left(\abs{\Bar{X}_n - p} > \varepsilon\right) \leq \frac{p(1-p)}{n\varepsilon^2} \leq \frac{1}{4n\varepsilon^2} \leq \delta \implies n \geq \frac{1}{4\varepsilon^2\delta}\]
Es decir, si queremos estar $\delta$ seguros de que el estimador está a una distancia menor que $\varepsilon$ de $p$, necesitamos que $n$ sea mayor que $\ds \frac{1}{4\varepsilon^2\delta}$.

\subsection{Cálculo de la distribución de la suma y el promedio}

Sean $\left(X_n\right)_{n\in \N}$ v.a. i.i.d. en $(\Omega, \F, P)$ con $X$ v.a. de referencia.
\begin{enumerate}
	\item \allbold{$X\sim \ber{p}$} $\implies S_n \sim \bin{n,p}$ y $Z_n \in \left\{0, \frac{1}{n}, \frac{2}{n}, \dots, 1\right\}$ con las mismas probabilidades que una $\bin{n,p}$.
	\item \allbold{$X\sim \poisson{\lambda}$} $\implies S_n \sim \poisson{n\lambda}$. Ya vimos que la suma de dos poisson es poisson. Ahora, por inducción asumimos que se cumple para $n-1$.
	      \[\begin{aligned}
			      P(S_n = k) & = P\left(\sum_{i=1}^{n} X_i = k\right) = \sum_{i=0}^k P\left(\sum_{i=1}^{n-1} X_i = k-i \we X_n = i\right)                                                                          \\
			                 & = \sum_{i=0}^k P\left(\sum_{i=1}^{n-1} X_i = k-i\right)P(X_n = i) = \sum_{i=0}^k \frac{(n-1)^{k-i} \lambda^{k-i}}{(k-i)!}e^{-(n-1)\lambda}\frac{\lambda^i}{i!}e^{-\lambda}          \\
			                 & = e^{-n\lambda} \lambda^k \sum_{i=0}^k \frac{(n-1)^{k-i}}{(k-i)!i!} = \frac{\lambda^k}{k!}e^{-n\lambda} \sum_{i=0}^k \binom{k}{i}(n-1)^{k-i} = \frac{(n\lambda)^k}{k!}e^{-n\lambda}
		      \end{aligned}\]
	\item \allbold{$X\sim \geom{p}$} $\implies S_n \sim \operatorname{binomial negativa}(n,p)$.
	      %   \[\begin{aligned}
	      % 	      P(S_n = k) & = P\left(\sum_{i=1}^{n} X_i = k\right) = \sum_{i=0}^{k} P\left(\sum_{i=1}^{n-1} X_i = k-i \we X_n = i\right) \\
	      % 	                 & = \sum_{i=0}^{k} p(1-p)^{k-i-1}p = \binom{k-1}{n-1}p^n(1-p)^{k-n}
	      %       \end{aligned}\]
	\item \allbold{$X\sim \normal{\mu, \sigma^2}$} $\implies S_n \sim \normal{n\mu, n\sigma^2}$.
	\item \allbold{$X\sim \EXP{\lambda}$} $\implies S_n \sim \operatorname{Gamma}(n,\lambda)$.
\end{enumerate}

\allbold{Técnicas generales} (Funciones generatrices)

\fecha{24/04/2024}

\subsection{Convergencia en distribución}

\begin{defn}[Convergencia en distribución]
	Sea $\left(X_n\right)_{n\in \N}$ una sucesión de v.a. y $X$ v.a. en el mismo espacio de probabilidad $(\Omega, \F, P)$,
	\[\left(X_n \xlongrightarrow{d} X\right) \iff \forall t \in \R^{(*)} : P\left(X_n \leq t\right) \xrightarrow{n\to\infty} F_X(t) \defeq P(X \leq t)\]
	\hspace*{\fill} $(*)$ $F_X$ debe ser continua en $t$.
\end{defn}

\subsubsection{Teorema del límite central / central del límite}

Vamos a tipificar $\ds S_n\defeq \sum_{i=1}^{n} X_i$ y $\ds Z_n \defeq \frac{1}{n}\sum_{i=1}^{n} X_i$ con $\left(X_n\right)_{n\in \N}$ i.i.d. con $X$ de referencia.
\[W_n \defeq \frac{S_n - n\mu}{\sigma \sqrt{n}} \quad \we \quad V_n \defeq \frac{Z_n - \mu}{\sigma/\sqrt{n}}\]

\begin{teo}[del límite central]
	Sean $\left(X_n\right)_{n\in \N}$ v.a. i.i.d. en $(\Omega, \F, P)$ con $X$ de referencia.
	\[\implies \forall t \in \R : P\left(\frac{S_n - n\mu}{\sigma \sqrt{n}} \leq t\right) \xrightarrow{n\to\infty} \Phi(t) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{t} e^{-\frac{x^2}{2}}dx\]
	Es decir, este teorema nos dice que $\ds \frac{S_n - n\mu}{\sigma \sqrt{n}} \xlongrightarrow{d} U$ donde $U\sim\normal{0,1}$
\end{teo}

\begin{ejem}
	Sea $X\sim \ber{p}$ la v.a. de referencia de $\left(X_n\right)_{n\in \N}$ i.i.d., entonces \\
	$S_n \sim \bin{n,p}$ y $Z_n \in \left\{0, \frac{1}{n}, \frac{2}{n}, \dots, 1\right\}$ con las mismas probabilidades que una $\bin{n,p}$.

	Si $p=\sfrac{1}{2}$ y $n=1000$ (lanzamos una moneda regular 1000 veces) y nos piden:
	\[P(480 \leq S_{1000} \leq 530) = \sum_{j=480}^{530} \binom{1000}{j} \left(\frac{1}{2}\right)^{1000} \approx 87.578\%\]
	Pero podemos aproximar la respuesta usando la normal por el teorema del límite central:
	\[\begin{aligned}
			P(480 \leq S_{1000} \leq 530) & = P\left(\frac{480-500}{\sqrt{250}} \leq \frac{S_{1000}-500}{\sqrt{250}} \leq \frac{530-500}{\sqrt{250}}\right) \\
			                              & \approx \Phi\left(\frac{30}{\sqrt{250}}\right) - \Phi\left(\frac{-20}{\sqrt{250}}\right) \approx 86.816\%
		\end{aligned}\]
\end{ejem}

\begin{ejem}
	Lanzamos un dado 10000 veces y nos piden $P(3400 \leq S \leq 3500)$.

	Tenemos $\ds X\sim \unif{1,6} \implies E(X) = \frac{7}{2} \we V(X) = \frac{35}{12}$ y,\\
	por tanto, $\ds S = \sum_{i=1}^{1000} X_i \implies E(S) = 3500 \we V(S) = 1000\frac{35}{12}$.
	\[\begin{aligned}
			P(3400 \leq S \leq 3500) & = P\left(\frac{-100}{\sqrt{1000 \cdot \sfrac{35}{12}}} \leq \frac{S-3500}{\sqrt{1000\cdot \sfrac{35}{12}}} \leq 0\right) \\
			                         & \approx \Phi(0) - \Phi\left(\frac{-100}{\sqrt{1000\cdot \sfrac{35}{12}}}\right) \approx 46.79\%
		\end{aligned}\]
\end{ejem}

\begin{ejem}[Intervalo de confianza]
	Sea $X\sim \ber{p}$ de referencia con $p$ desconocido de $\left(X_n\right)_{n\in\N}$ i.i.d. y $\overline{X}_n$ el promedio de las $X_i$.

	Fijamos $\alpha \in \R$ ``pequeño'' y definimos $\ds z_{\alpha/2} \defeq \Phi^{-1}\left(1-\frac{\alpha}{2}\right)$.
	\[\begin{aligned}
			\implies & P\left(-z_{\alpha/2} \leq \frac{\overline{X}_n - p}{\sqrt{\sfrac{p(1-p)}{n}}} \leq z_{\alpha/2}\right) \approx 1-\alpha                           \\
			=        & P\left(-\frac{-z_{\alpha/2}\sqrt{p(1-p)}}{\sqrt{n}} \leq \overline{X}_n - p \leq \frac{z_{\alpha/2}\sqrt{p(1-p)}}{\sqrt{n}}\right)                \\
			=        & P\left(\overline{X}_n - \frac{z_{\alpha/2}\sqrt{p(1-p)}}{\sqrt{n}} \leq p \leq \overline{X}_n + \frac{z_{\alpha/2}\sqrt{p(1-p)}}{\sqrt{n}}\right)
		\end{aligned}\]
	Tenemos confianza $1-\alpha$ de que $p$ está en el intervalo $\left(\overline{x}_n - \frac{z_{\alpha/2}\sqrt{p(1-p)}}{\sqrt{n}}, \overline{x}_n + \frac{z_{\alpha/2}\sqrt{p(1-p)}}{\sqrt{n}}\right)$ donde $\overline{x}_n$ es el valor observado de $\overline{X}_n$. Sin embargo, este intervalo depende de $p$, pero podemos acotarlo por $\ds \left(\overline{x}_n - \frac{z_{\alpha/2}}{2\sqrt{n}}, \overline{x}_n + \frac{z_{\alpha/2}}{2\sqrt{n}}\right)$.

	Por tanto, el error sería $\ds \frac{z_{\alpha/2}}{2\sqrt{n}}$. Si queremos que sea menor que $\varepsilon$, necesitamos que $\ds n \geq \frac{\left(z_{\alpha/2}\right)^2}{4\varepsilon^2}$.
\end{ejem}

\fecha{25/04/2024}

\begin{ejer}[4 b del examen]
	Sea $(X, Y) \sim \operatorname{2-dim}\normal{0,1}$ con correlación $\rho$. \\
	Calcula la varianza de $X\cdot Y$. Tenemos que
	\[V(X\cdot Y) = E\left(X^2\cdot Y^2\right) - E\left(X\cdot Y\right)^2 = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x^2y^2\frac{1}{2\pi\sqrt{1-\rho^2}}e^{-\frac{1}{2(1-\rho^2)}(x^2-2\rho xy + y^2)}\odif{x} \odif{y} - \rho^2\]
	porque $E(X\cdot Y) = \rho$.
	Completando cuadrados, obtenemos
	\[\begin{aligned} %TODO: Terminar ejercicio / revisar
			V(X\cdot Y) & = \frac{1}{2\pi\sqrt{1-\rho^2}}\int_{-\infty}^{\infty} x^2 e^{-\frac{x^2}{2}} \left(\int_{-\infty}^{\infty} y^2 e^{-\frac{1}{2}\left(\frac{y-\rho x}{\sqrt{1-\rho^2}}\right)^2}\odif{y}\right)\odif{x} -                     \rho^2           \\
			            & = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} x^2 e^{-\frac{x^2}{2}} \left(\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}\sqrt{1-\rho^2}}y^2 e^{-\frac{1}{2}\left(\frac{y-\rho x}{\sqrt{1-\rho^2}}\right)^2}\odif{y}\right) \odif{x} - \rho^2
		\end{aligned}\]
	El término entre paréntesis es $E\left(W^2\right)$ donde $W\sim\normal{\rho x, 1-\rho^2}$ y como \\$V(W) = E(W^2) - E(W)^2 = 1-\rho^2$, tenemos $E(W^2) = V(W) + E(W)^2 = 1-\rho^2 + (\rho x)^2$.
	\[\begin{aligned}
			\implies \lbox{V(X\cdot Y)} & = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} x^2 e^{-\frac{x^2}{2}} \left(1-\rho^2 + \rho^2 x^2\right)\odif{x} - \rho^2                                                             \\
			                            & = (1-\rho^2) \int_{-\infty}^{\infty} x^2 \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}\odif{x} + \rho^2 \int_{-\infty}^{\infty} x^4 \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}\odif{x} - \rho^2 \\
			                            & = (1-\rho^2) + \rho^2 E\left(\left(\normal{0, 1}\right)^4\right) - \rho^2 = 1-\rho^2 + \rho^2\left(0^4 +6\cdot 0^2 1^2 + 3(1^4)\right) - \rho^2                                        \\
			                            & = 1 - \rho^2 + 3\rho^2 - \rho^2 = \rbox{1 + \rho^2}
		\end{aligned}\]
\end{ejer}

\fecha{29/06/2024}

\subsubsection{Variaciones del TCL}

Si consideramos $\left(X_n\right)_{n\in \N}$ v.a. independientes no idénticas con $E(X_i) = \mu_i \we V(X_i) = \sigma_i^2$
\[\implies \tex{La suma tipificada es } T_n \defeq \frac{\frac{1}{n} \sum_{i=1}^{n} X_i - \frac{1}{n}\sum_{i=1}^{n} \mu_i}{\sqrt{\frac{1}{n^2}\sum_{i=1}^{n} \sigma_i^2}} = \frac{\sum_{i=1}^{n} X_i - \mu_i}{\sqrt{\sum_{i=1}^{n} \sigma_i^2}}\]
Ahora, según las condiciones de Lyapunov, definimos $\ds S_n^\alpha \defeq \sum_{i=1}^{n} \sigma_i^\alpha$, entonces
\[\exists \delta > 0 : \frac{1}{S_n^{2+\delta}} \sum_{i=1}^{n} E\left(\abs{X_i - \mu_i}^{2+\delta}\right) \xrightarrow{n\to\infty} 0 \implies T_n \xlongrightarrow{d} \normal{0,1}\]

\begin{teo}[Cota de Berry-Esseen]
	Sea $\left(X_n\right)_{n\in \N}$ una sucesión de v.a. i.i.d. con $E(X) = \mu \we V(X) = \sigma^2 < \infty$ y $E\left(\abs{X}^3\right) < \infty$.
	\[\implies \sup_{t\in \R} \abs{P\left(\frac{\sum X_i - n\mu}{\sigma \sqrt{n}} \leq t\right) - \Phi(t)} \leq C \frac{E\left(\abs{X}^3\right)}{\sigma^3}\frac{1}{\sqrt{n}} \tex{ con } C \tex{ constante universal}\]
\end{teo}

\subsection{Funciones generatrices de momentos}

\allbold{Recordamos:} Sea $X$ v.a. que toma valores en $\{0, 1, \dots\}$ con probabilidades $p_0, p_1, \dots$, entonces la función generatriz de probabilidad de $X$ es $\ds G_X(s) \defeq \sum_{n=0}^{\infty} p_n s^n$. % = E\left(s^X\right)$.

\begin{defn}
	Sea $X$ v.a., su función generatriz de momentos  es $\ds M_X(t) \defeq E\left(e^{tX}\right)$.
\end{defn}

\begin{obs}
	\begin{itemize}
		\item Si $X$ toma valores en $\{0, 1, \dots\}$, entonces $M_X(t) = G_X(e^t)$.
		      \[\tex{porque }M_X(t) = E\left(e^{tX}\right) = \sum_{n=0}^{\infty} e^{tn}p_n = \sum_{n=0}^{\infty} p_n (e^t)^n = G_X(e^t)\]
		\item Si $X$ es discreta $\ds \implies E\left(e^{tX}\right) = \sum_{x \in X(\Omega)} e^{tx}P(X=x)$.

		      Si $X$ es continua $\ds \implies E\left(e^{tX}\right) = \int_{-\infty}^{\infty} e^{tx}f_X(x)\odif{x}$.
	\end{itemize}
\end{obs}

\begin{ejem}
	\begin{itemize}
		\item $\ds X\sim \EXP{\lambda} \implies M_X(t) = \int_{0}^{\infty} e^{tx}\lambda e^{-\lambda x}\odif{x} = \begin{cases}
				      \frac{\lambda}{\lambda - t} & \tex{ si } t < \lambda    \\
				      \infty                      & \tex{ si } t \geq \lambda
			      \end{cases}$
		      \[\tex{porque } E\left(e^{tX}\right) = \int_{0}^{\infty} e^{tx}\lambda e^{-\lambda x}\odif{x} = \lambda \int_{0}^{\infty} e^{-(\lambda - t)x}\odif{x} = \begin{cases}
				      \frac{\lambda}{\lambda - t} & \tex{ si } t < \lambda    \\
				      \infty                      & \tex{ si } t \geq \lambda
			      \end{cases}\]
		\item $\ds X\sim \normal{0,1} \implies M_X(t) = \int_{-\infty}^{\infty} e^{tx}\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}\odif{x} = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty} e^{tx-\frac{x^2}{2}}\odif{x}$
		      \[\implies M_X(t) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-\frac{1}{2}\left(x-t\right)^2} e^{\frac{t^2}{2}}\odif{x} = e^{\frac{t^2}{2}} \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-\frac{1}{2}\left(x-t\right)^2}\odif{x} = e^{\frac{t^2}{2}}\]
	\end{itemize}
\end{ejem}

\fecha{30/04/2024}

\begin{ejem}
	$X$ sigue una distribución de Cauchy $\iff \forall x \in \R : f_X(x) = \frac{1}{\pi} \frac{1}{1+x^2}$.
	\[\implies M_X(t) = \int_{-\infty}^{\infty} e^{tx} \frac{1}{\pi} \frac{1}{1+x^2}\odif{x} \tex{ que solo converge cuando }t=0\]
	Es decir, muchas veces $M_X$ genera problemas porque $e^{tX}$ se hace grande. \\
	Por tanto, se define la transformada de Fourier de $X$ como $\ds \hat{f}_X(t) \defeq \int_{-\infty}^{\infty} e^{itx}f_X(x)\odif{x}$. En este caso, $\forall t, x \in \R : \abs{e^{itx}} = 1$.
\end{ejem}

\subsubsection{¿Por qué ese nombre?}

Si tenemos en cuenta el desarrollo de Taylor de $\ds e^{z} = \sum_{n=0}^{\infty} \frac{z^n}{n!}$, entonces
\[\begin{aligned}
		\forall t\in \R : M_X(t) & = E\left(e^{tX}\right) = E\left(1 + tx + \frac{t^2x^2}{2} + \frac{t^3x^3}{3!} + \dots\right)                           \\
		                         & = 1 + tE(X) + \frac{t^2E(X^2)}{2} + \frac{t^3E(X^3)}{3!} + \dots = \sum_{n=0}^{\infty} \frac{E\left(X^n\right)t^n}{n!}
	\end{aligned}\]
Es decir, $M_X(t)$ es función generatriz (exponencial) de la sucesión de momentos.

\begin{teo}
	Sea $X$ v.a. con función generatriz de momentos $M_X(t)$.
	\[M_X(t) \tex{ converge en }\abs{t}< \delta \tex{ para cierto }\delta \implies \forall k \in \Z_{\geq 0} : E\left(X^k\right) = M^{(k)}_X (0)\]
\end{teo}

\begin{ejem}
	Sea $X\sim\normal{0,1} \implies M_X(t) = e^{\frac{t^2}{2}}$.
	\[\begin{aligned}
			M'_X(t)      & = te^{\frac{t^2}{2}}                                                & \implies M'_X(0) = 0      \\
			M''_X(t)     & = e^{\frac{t^2}{2}} + t^2e^{\frac{t^2}{2}}                          & \implies M''_X(0) = 1     \\
			M'''_X(t)    & = 2te^{\frac{t^2}{2}} + t^3e^{\frac{t^2}{2}}                        & \implies M'''_X(0) = 0    \\
			M^{(4)}_X(t) & = 3e^{\frac{t^2}{2}} + 3t^2e^{\frac{t^2}{2}} + t^4e^{\frac{t^2}{2}} & \implies M^{(4)}_X(0) = 3
		\end{aligned}\]
\end{ejem}

\subsubsection{La gracia}

\begin{itemize}
	\item Sea $X$ v.a. con función generatriz de momentos $M_X(t)$. Definimos $Y\defeq aX+b$ con $a,b\in \R \implies M_Y(t) = E\left(e^{tY}\right) = E\left(e^{t(aX+b)}\right) = e^{tb}E\left(e^{(at)X}\right) = e^{tb}M_X(at)$.

	\item Sean $X$ e $Y$ v.a. independientes con funciones generatrices de momentos $M_X(t)$ y $M_Y(t) \implies M_{X+Y}(t) = E\left(e^{t(X+Y)}\right) = E\left(e^{tX}e^{tY}\right) = E\left(e^{tX}\right)E\left(e^{tY}\right) = M_X(t)M_Y(t)$.

	      Si, en su lugar, tenemos $X_1, \dots, X_n$ v.a. independientes con funciones generatrices de momentos $M_{X_i}(t)$, entonces $M_{X_1 + \dots + X_n}(t) = M_{X_1}(t) \cdots M_{X_n}(t)$.
\end{itemize}

\subsubsection{El problema de los momentos}

Supongamos que para $X$ v.a. tenemos todos sus momentos, ¿podemos recuperar $X$?

La respuesta es que no. Por ejemplo, si $X$ sigue una distribución dada por la función de densidad $\ds \forall x > 0 : f_X(x) = \frac{1}{x\sqrt{2\pi}} e^{-\frac{1}{2}\ln^2(x)}$.

Entonces, la familia de variables aleatorias $\{X_a : a \in (-1, 1)\}$ dada por la familia de funciones de densidad $\left\{f_{X_a}(x) = (1+a\sin{(2\pi \ln{x}) f_X(x)} : a \in (-1, 1)\right\}$ tiene los mismos momentos. %TODO: Comprobarlo

\begin{teo}(Unicidad)
	Sea $M_X(t) = E\left(e^{tX}\right) < \infty$ para $\abs{t} < \delta$ para cierto $\delta > 0$.
	\[\implies \exists ! X \tex{ v.a. con función generatriz de momentos }M_X\]
	En este caso, además, $\forall k \in \Z_{\geq 0} : E\left(X^k\right) < \infty$ y $\ds \forall t : \abs{t} < \delta : M_X(t) = \sum_{k=0}^{\infty} \frac{E\left(X^k\right)t^k}{k!}$.
\end{teo}

\begin{ejem}
	Sean $X_1\sim \normal{\mu_1, \sigma_1^2}$ y $X_2\sim \normal{\mu_2, \sigma_2^2}$ v.a. independientes.
	\[\begin{aligned}
			 & \implies M_{X_1+X_2}(t) = M_{X_1}(t)M_{X_2}(t) = e^{\mu_1t + \frac{1}{2}\sigma_1^2t^2}e^{\mu_2t + \frac{1}{2}\sigma_2^2t^2} = e^{(\mu_1+\mu_2)t + \frac{1}{2}(\sigma_1^2+\sigma_2^2)t^2} \\
			 & \implies X_1 + X_2 \sim \normal{\mu_1+\mu_2, \sigma_1^2+\sigma_2^2}
		\end{aligned}\]

	Sean $a, b \in \R$, definimos $Z\defeq aX_1 + bX_2$.
	\[\begin{aligned}
			 & \begin{aligned}
				   \implies M_Z(t) & = M_{aX_1+bX_2}(t) = M_{X_1}(at)M_{X_2}(bt) = e^{a\mu_1t + \frac{1}{2}a^2\sigma_1^2t^2}e^{b\mu_2t + \frac{1}{2}b^2\sigma_2^2t^2} \\
				                   & = e^{(a\mu_1+b\mu_2)t + \frac{1}{2}(a^2\sigma_1^2+b^2\sigma_2^2)t^2}
			   \end{aligned} \\
			 & \implies Z \sim \normal{a\mu_1+b\mu_2, a^2\sigma_1^2+b^2\sigma_2^2}
		\end{aligned}\]
\end{ejem}

\fecha{06/05/2024}

\begin{dem}[del TCL]
	Sean $\left(X_n\right)_{n\in \N}$ v.a. i.i.d. con $X$ de referencia y $E(X) = \mu \we V(X) = \sigma^2$.

	Suponemos, \underline{además}, que $M_X(t)$ está bien definida en $\abs{t} < \delta$ para cierto $\delta > 0$. (esto no estaba entre las hipótesis del teorema).

	Definimos $\ds \left(U_n\right)_{n\in \N}$ como $\ds \forall n \in \N : U_n \defeq X_n - \mu \implies E(U_n) = 0 \we V(U_n) = \sigma^2$ y son i.i.d.

	Definimos $\ds S_n \defeq \sum_{i=1}^{n} X_i \implies E(S_n) = n\mu \we V(S_n) = n\sigma^2$
	\[\implies \til{S}_n = \frac{\sum_{i=1}^{n} X_i - n\mu}{\sigma\sqrt{n}} = \frac{1}{\sigma\sqrt{n}}\sum_{i=1}^{n} U_i \implies E(\til{S}_n) = 0 \we V(\til{S}_n) = 1\]
	\[\implies M_{\til{S}_n}(t) = E\left(e^{t\til{S}_n}\right) = E\left(e^{\frac{t}{\sigma\sqrt{n}}\sum_{i=1}^{n} U_i}\right) = E\left(e^{\frac{t}{\sigma\sqrt{n}}U_1} \cdots e^{\frac{t}{\sigma\sqrt{n}}U_n}\right) = \left(M_U\left(\frac{t}{\sigma\sqrt{n}}\right)\right)^n\]
	\[M_U(x) = \sum_{k=0}^{\infty} \frac{E(U^k)}{k!}x^k = 1 + \cancelto{0}{E(U)} x + \frac{E(U^2)}{2}x^2 + o(x^2) = 1 + \frac{\sigma^2}{2}x^2 + o(x^2)\]
	\[\implies \forall t \in \R : M_{\til{S}_n}(t) = \left(1 + \frac{\cancel{\sigma^2}}{2}\frac{t^2}{\cancel{\sigma^2}n} + o\left(\frac{1}{n}\right)\right)^n = \left(1 + \frac{\sfrac{t^2}{2}}{n} + o\left(\frac{1}{n}\right)\right)^n \xrightarrow{n\to\infty} e^{\frac{t^2}{2}}\]
	Como $e^{\frac{t^2}{2}}$ es la función generatriz de momentos de una $\normal{0,1}$, entonces $\til{S}_n \xlongrightarrow{d} \normal{0,1}$.
\end{dem}

Para que la demostración anterior fuese válida, habría que primero haber demostrado el siguiente teorema.
\begin{teo}
	Sean $Z_1, Z_2, \dots$ v.a. con funciones generatrices de momentos $M_{Z_i}(t)$ definidas entorno al origen. Si $\forall t \in \R : \lim_{n\to\infty} M_{Z_n}(t) = M(t)$, entonces $exists Z$ v.a. con función generatriz de momentos $M(t) : Z_n \xlongrightarrow{d} Z$
\end{teo}

\subsection{Función característica}

\begin{defn}[Función característica]
	Sea $X$ v.a., $\forall t \in \R : \phi_X(t) \defeq E\left(e^{itX}\right)$.
\end{defn}

\allbold{¿Cómo se calcula?} Suponemos que $X$ tiene función de densidad $f_X(x)$.
\[\implies E\left(e^{itX}\right) = \int_{-\infty}^{\infty} e^{itx}f_X(x)\odif{x} \tex{ que es una función de $\R$ a $\C$}\]

\begin{ejem}
	Sea $X\sim\ber{p}$ con $p\in(0,1)$.
	\[\implies \phi_X(t) = E\left(e^{itX}\right) = e^{it\cdot 0}(1-p) + e^{it\cdot 1}p = (1-p) + p e^{it}\] %TODO: Dibujo
\end{ejem}

\allbold{¿Por qué esto mola?}
\begin{enumerate}
	\item $\ds \forall t \in \R : \abs{\phi_X(t)} \leq 1$.
	\item Siempre está bien definida.
	\item Sean $X$ e $Y$ v.a. independientes con funciones características $\phi_X(t)$ y $\phi_Y(t)$.
	      \[\implies \phi_{aX+b}(t) = E\left(e^{it(aX+b)}\right) = e^{itb}E\left(e^{iatX}\right) = e^{itb}\phi_X(at)\]
	      \[\implies \phi_{X+Y}(t) = E\left(e^{it(X+Y)}\right) = E\left(e^{itX}e^{itY}\right) = E\left(e^{itX}\right)E\left(e^{itY}\right) = \phi_X(t)\phi_Y(t)\]
	\item (Unicidad) Sean $X$ e $Y$ con funciones características $\phi_X(t)$ y $\phi_Y(t)$.
	      \[\forall t \in \R :  \phi_X(t) = \phi_Y(t) \iff \tex{$X$ e $Y$ tienen la misma distribución}\]
	\item (Continuidad) Sean $\left(Z_n\right)_{n\geq 1}$ v.a. con funciones características $\phi_{Z_n}(t)$.
	      \[z_n \xrightarrow{d} Z \iff \forall t \in \R : \lim_{n\to\infty} \phi_{Z_n}(t) = \phi_Z(t)\]
\end{enumerate}

\begin{ejem}
	\begin{itemize} %TODO: Comprobar los resultados
		\item $\ds X\sim\EXP{\lambda} \implies \phi_X(t) = \int_{0}^{\infty} e^{itx}\lambda e^{-\lambda x}\odif{x} = \frac{\lambda}{\lambda - it}$
		\item $\ds X\sim\normal{0,1} \implies \phi_X(t) = \int_{-\infty}^{\infty} e^{itx}\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}\odif{x} = e^{-\frac{t^2}{2}}$
	\end{itemize}
\end{ejem}

\fecha{07/05/2024}

\[\phi_X(t) = E\left(e^{itX}\right) = \int_{-\infty}^{\infty} e^{itx}f_X(x)\odif{x} = \int_{-\infty}^{\infty} \cos(tx)f_X(x)\odif{x} + i\int_{-\infty}^{\infty} \sin(tx)f_X(x)\odif{x}\]

\begin{teo}
	Sea $X$ v.a. con función generatriz de momentos $M_X(t)$ definida en un entorno del origen $\implies \phi_X(t) = M_X(it)$.
\end{teo}

Por ejemplo, $\ds X \sim \normal{\mu, \sigma^2} \implies \phi_X(t) = e^{i\mu t - \frac{\sigma^2t^2}{2}}$.

\begin{dem}[del TCL]
	Sean $\left(X_n\right)_{n\in \N}$ v.a. i.i.d. con $X$ de referencia y $E(X) = \mu \we V(X) = \sigma^2$.
	\[\phi_{Z_n}(t) = \phi_{S_n}\left(\frac{t}{n}\right) = \phi_X\left(\frac{t}{n}\right)^n\]
	Desarrollando por Taylor tenemos: $\ds \phi_X(w) = 1 + it E(X) + o(w)$.
	\[\implies \phi_{Z_n}(t) = \left(1 + \frac{it\mu}{n} + o\left(\frac{1}{n}\right)\right)^n \xrightarrow{n\to\infty} e^{it\mu}\]
	Si $X$ es una v.a. constante $(X=\mu) \implies \phi_X(t) = e^{it\mu}$. Por tanto, $Z_n \xlongrightarrow{d} \mu$.

	Ahora, podemos demostar la ley débil de los grandes números.\\
	Basta ver que $Z_n \xlongrightarrow{d} \mu \implies Z_n \xlongrightarrow{P} \mu$ cuando $\mu$ es constante (ejs 7 y 9 de la hoja 5).

	Definimos $U_n \defeq X_n - \mu \implies E(U_n) = 0 \we V(U_n) = \sigma^2$ y son i.i.d. Estudiemos
	\[ \til{S}_n \defeq \frac{\sum_{j=1}^n X_j - n\mu}{\sigma \sqrt{n}} = \frac{1}{\sigma\sqrt{n}}\sum_{j=1}^n U_j \implies \phi_{\til{S}_n}(t) = \left(\phi_U\left(\frac{t}{\sigma\sqrt{n}}\right)\right)^n\]
	Por Taylor tenemos $\ds \phi_U(w) = 1 + iw E(U) + \frac{(iw)^2}{2!}E(U^2) + o(w^2) $.
	\[\implies \phi_{\til{S}_n}(t) = \left(1 - \frac{t^2}{2n} + o\left(\frac{1}{n}\right)\right)^n \xrightarrow{n\to\infty} e^{-\frac{t^2}{2}}\]
\end{dem}

\subsubsection{Función característica para distribución de Cauchy: Variable compleja}

Sea $X\sim\operatorname{Cauchy} \iff \forall x \in \R : f_X(x) = \frac{1}{\pi} \frac{1}{1+x^2}$. Queremos calcular su función característica.
\[\implies \phi_X(t) = \int_{-\infty}^{\infty} e^{itx} \frac{1}{\pi} \frac{1}{1+x^2}\odif{x} = \cdots = e^{-\abs{t}}\]

Veamos una técnica de variable compleja para calcular la integral. Consideramos $t>0$.\\
Definimos $\ds \forall z \in \C : g(z) \defeq \frac{e^{itz}}{1+z^2}$ con un $t\in\R_{>0}$ fijo. Esta función tiene dos polos en $i$ y $-i$ y hay un teorema en variable compleja que dice que si $f$ es holomorfa en un entorno de un camino cerrado $\gamma$ y $f$ no tiene polos en el interior de $\gamma$, entonces $\int_{\gamma} f(z)\odif{z} = 0$. \\
Si, en su defecto, $f$ tiene un polo en $z_0$
\[\implies \int_{\gamma} f(z)\odif{z} = 2\pi i \operatorname{Res}(f, z_0) \tex{ donde }\operatorname{Res}(f, z_0) = \lim_{z\to z_0} (z-z_0)f(z)\]

Definimos $\Gamma_R \subset \C$ como el camino (orientado) que va de $-R$ a $R$ por la recta real y luego vuelve a $-R$ por la semicircunferencia superior de radio $R$ en $\C$ donde $R\in \R_{>1}$. Podemos separar este camino en dos partes: $\Gamma_R = \Gamma_R^1 \cup \Gamma_R^2$ donde $\Gamma_R^1$ es la parte de la recta real y $\Gamma_R^2$ es la semicircunferencia superior y se cumple que:
\[\int_{\Gamma_R} g(z)\odif{z} = \int_{\Gamma_R^1} g(z)\odif{z} + \int_{\Gamma_R^2} g(z)\odif{z} = 2\pi i \operatorname{Res}(g, i)\]
Tenemos $\ds \operatorname{Res}(g, i) = \lim_{z\to i} \cancel{(z-i)}\frac{e^{itz}}{(z+i)\cancel{(z-i)}} = \frac{e^{-t}}{2i} \implies \int_{\Gamma_R} g(z)\odif{z} = 2\pi i \frac{e^{-t}}{2i} = \pi e^{-t}$.

Observamos que, claramente, $\ds \lim_{R\to\infty} \int_{\Gamma_R^1} g(z)\odif{z} = \int_{-\infty}^{\infty} e^{itx} \frac{1}{1+x^2}\odif{x} = \pi \cdot \phi_X(t)$. Ahora
\[\abs{\int_{\Gamma_R^2} g(z)\odif{z}} \leq \int_{\Gamma_R^2} \abs{g(z)}\odif{z} \leq \pi R \max_{z\in \Gamma_R^2} \abs{g(z)} = \pi R \max_{z\in \Gamma_R^2} \abs{\frac{e^{itz}}{1+z^2}}\]
Como $z \in \Gamma_R^2 \implies \abs{z} = R$, entonces $\ds \abs{e^{itz}} \leq \abs{e^{it R e^{i\theta}}} = \abs{e^{itR\cos{\theta}} e^{-tR\sin{\theta}}}$, y como $t>0$
\[-tR\sin{\theta} \leq 0 \implies \abs{\frac{e^{itz}}{1+z^2}} \leq \frac{\abs{e^{itR\cos{\theta}}}}{\abs{1+z^2}} \leq \frac{1}{\abs{1+z^2}} \]
Ahora, dado que $\abs{1+z^2} \geq \abs{z^2} - 1 = R^2 - 1$, tenemos $\ds \abs{\frac{e^{itz}}{1+z^2}} \leq \frac{1}{R^2 - 1}$.
\[\implies \abs{\int_{\Gamma_R^2} g(z)\odif{z}} \leq \pi R \max_{z\in \Gamma_R^2} \abs{\frac{e^{itz}}{1+z^2}} \leq \frac{\pi R}{R^2 - 1} \implies \lim_{R\to\infty} \int_{\Gamma_R^2} g(z)\odif{z} = 0\]
Concluimos que para todo $t$ positivo, $\phi_X(t) = e^{-t}$:
\[ \pi e^{-t} = \lim_{R\to \infty} \int_{\Gamma_R} g(z)\odif{z} = \lim_{R\to\infty} \left[\int_{\Gamma_R^1} g(z)\odif{z} + \int_{\Gamma_R^2} g(z)\odif{z}\right] = \pi \phi_X(t) + 0 \implies \phi_X(t) = e^{-t}\]
Para $t<0$ se puede hacer un razonamiento análogo con un $\Gamma$ que vaya por la semicircunferencia inferior y luego vuelva por la recta real. Finalmente llegamos a
\[\forall t > 0 : \phi(t) = e^{-t} \, \, \we \, \, \forall t < 0 : \phi(t) = e^t \implies \boxed{\forall t \in \R : \phi(t) = e^{-\abs{t}}}\]

Como además, tenemos $\ds \phi(t) = \int_{-\infty}^{\infty} e^{itx} \frac{1}{\pi} \frac{1}{1+x^2}\odif{x} = \frac{1}{\pi}\int_{-\infty}^{\infty} \frac{\cos(tx)}{1+x^2}\odif{x} + i\frac{1}{\pi}\int_{-\infty}^{\infty} \frac{\sin(tx)}{1+x^2}\odif{x}$
\[\implies \int_{-\infty}^{\infty} \frac{\cos(tx)}{1+x^2}\odif{x} = \pi e^{-\abs{t}} \,\, \we \,\, \int_{-\infty}^{\infty} \frac{\sin(tx)}{1+x^2}\odif{x} = 0\]