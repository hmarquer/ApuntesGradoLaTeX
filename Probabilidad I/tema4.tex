\fecha{22/04/2024}

\section{Convergencia de variables aleatorias}

Partimos de una sucesión de variables aleatorias en un espacio de probabilidad $(\Omega, \F, P)$, $\ds \left(X_n\right)_{n \in \N}$, y queremos estudiar las series
\[\big(S_n\big)_{n\in \N} = \left(\sum_{i=1}^{n} X_i\right)_{n \in \N} \quad \we \quad \big(Z_n\big)_{n\in \N} = \left(\frac{1}{n}\sum_{i=1}^{n} X_i\right)_{n \in \N}\]
Interesan, específicamente, los límites cuando $n\to \infty$ de estas series.
\begin{enumerate}
	\item Sabemos que significa $\ds \lim_{n\to\infty} E(S_n)$ y $\ds \lim_{n\to\infty} V(S_n)$.

	      Pero, ¿qué significa $\ds \lim_{n\to\infty} S_n = S$? Requerimos de técnicas más avanzadas que se denominan \textbf{modos de convergencia}.
	\item Casi siempre asumiremos que $\{X_n\}_{n\in \N}$ son \textbf{independientes e idénticamente distribuidas (i.i.d.)}.
	      \begin{itemize}
		      \item Todas las $X_i$ tienen la misma distribución. Por tanto, es habitual definir una \allbold{$X$ de referencia} que tenga la misma distribución que todas las $X_i$.
		      \item Las $X_i$ son completamente independientes, es decir, para todo subconjunto finito $I \subset \N$, $\{X_i\}_{i\in I}$ son independientes.
	      \end{itemize}
	\item Descubriremos que, para $n$ grande y $X$ de referencia, $Z_n$ se comporta como $E(X)$. Además, veremos el teorema central del límite, que nos dice que $\ds \frac{Z_n-E(X)}{\sfrac{\sigma(Z_n)}{\sqrt{n}}} \approx \normal{0,1}$.
\end{enumerate}
\subsection{Medias y varianzas de las sumas y las medias}
\[\begin{aligned}
		E(S_n) & = E\left(\sum_{i=1}^{n} X_i\right) = \sum_{i=1}^{n} E(X_i) \stackrel{(\star)}{=} nE(X)                      \\
		E(Z_n) & = E\left(\frac{1}{n}\sum_{i=1}^{n} X_i\right) = \frac{1}{n}\sum_{i=1}^{n} E(X_i) \stackrel{(\star)}{=} E(X)
	\end{aligned}\]
\hspace*{\fill} $(\star)$ Si $X_i$ tienen la misma media.

\[\begin{aligned}
		V(S_n) & = V\left(\sum_{i=1}^{n} X_i\right) = V(X_1) + V\left(\sum_{i=2}^{n} X_i\right) + 2 \cov{X_1, \sum_{i=2}^{n} X_i}                                                            \\
		       & = V(X_1) + 2 \sum_{i=2}^{n} \cov{X_1, X_i} + V\left(\sum_{i=2}^{n} X_i\right)                                                                                               \\
		       & = V(X_1) + 2 \sum_{i=2}^{n} \cov{X_1, X_i} + V(X_2) + 2 \sum_{i=3}^{n} \cov{X_2, X_i} + V\left(\sum_{i=3}^{n} X_i\right)                                                    \\
		       & = \sum_{i=1}^{n} V(X_i) + 2 \sum_{1\leq i < j \leq n} \cov{X_i,X_j} \stackrel{(*)_1}{=} \sum_{i=1}^{n} V(X_i) \stackrel{(*)_2}{=} n(\sigma(X))^2                            \\
		V(Z_n) & = V\left(\frac{1}{n}\sum_{i=1}^{n} X_i\right) \stackrel{(*)_1}{=} \frac{1}{n^2}\sum_{i=1}^{n} V(X_i) \stackrel{(*)_2}{=} \frac{(\sigma(X))^2}{n} \xrightarrow{n\to\infty} 0
	\end{aligned}\]
\hspace*{\fill} $(*)_1$ Si $X_i$ incorreladas, $(*)_2$ Si $X_i$ tienen la misma varianza.

\subsection{Convergencia cuadrática}

\begin{defn}[Convergencia cuadrática]
	Sean $\left(X_n\right)_{n\in \N}$ v.a. en $(\Omega, \F, P)$ y $X$ v.a. en $(\Omega, \F, P)$, $\left(X_n\right)_{n\in \N}$ \textbf{converge cuadráticamente} a $X$ $\left(X_n \xrightarrow{\tex{cuad.}} X\right)$
	\[\iff E\left(\abs{X_n - X}^2\right) \xrightarrow{n\to\infty} 0\]
\end{defn}

\begin{teo}
	Sean $\left(X_n\right)_{n\in \N}$ v.a. i.i.d. en $(\Omega, \F, P)$ con $X$ v.a. de referencia
	\[\implies Z_n\xrightarrow{\tex{cuad.}} E(X)\]
	\begin{dem}
		Si denominamos $\mu = E(X) \we V(X) = \sigma^2$, entonces
		\[V(Z_n)=\frac{\sigma^2}{n} \implies E\left(\abs{Z_n - \mu}^2\right) = V(Z_n) \xrightarrow{n\to\infty} 0\]
	\end{dem}
	Quitando hipótesis: basta con que sean incorreladas y tengan la misma media y varianza.
\end{teo}

\begin{teo}
	Sean $\left(X_n\right)_{n\in \N}$ v.a. en $(\Omega, \F, P)$ con $E(X_i) = \mu_i$ y $V(X_i) = \sigma_i^2$. Suponemos que $\ds \mu_n \defeq \frac{1}{n}\sum_{i=1}^{\infty} \mu_i \to \mu \in \R$ y $\ds V(Z_n) = \frac{1}{n^2}\sum_{i=1}^{\infty} \sigma_i^2 \to 0 \implies Z_n \xrightarrow{\tex{cuad.}} \mu$
	\begin{dem} (Buen ejercicio de examen)%TODO: terminar demostración
		\[\begin{aligned}
				E\left(\abs{Z_n - \mu}^2\right) & = E\left(\abs{Z_n - \mu_n + \mu_n - \mu}^2\right)                                                \\
				                                & = E\left(\abs{Z_n - \mu_n}^2 + \abs{\mu_n - \mu}^2 + 2(Z_n - \mu_n)(\mu_n - \mu)\right)          \\
				                                & = E\left((Z_n - \mu_n)^2\right) + {(\mu_n - \mu)^2} + 2(\mu_n - \mu)\cancelto{0}{E(Z_n - \mu_n)} \\
				                                & = E\left((Z_n - \mu_n)^2\right) + (\mu_n - \mu)^2 \xrightarrow{n\to\infty} 0
			\end{aligned}\]
	\end{dem}
\end{teo}

\subsection{Convergencia en probabilidad (ley débil)}

J.Bernoulli (1713) Ars Conjectandi: Si tienes un dado regular, cuantas más veces lo lances, más se aproximará la frecuencia relativa de un número a su probabilidad.

\fecha{23/04/2024}
\begin{defn}[Convergencia en probabilidad]
	Sean $\left(X_n\right)_{n\in \N}$ v.a. en $(\Omega, \F, P)$ y $X$ v.a. en $(\Omega, \F, P)$, $\left(X_n\right)_{n\in \N}$ \textbf{converge en probabilidad} a $X$ $\left(X_n \xlongrightarrow{P} X\right)$
	\[\iff \forall \varepsilon > 0 : \lim_{n\to\infty} P\left(\abs{X_n - X} > \varepsilon\right) = 0\]
\end{defn} %TODO: Añadir dibujo

\begin{teo}[Ley débil de los grandes números]
	Sean $\left(X_n\right)_{n\in \N}$ v.a. i.i.d. en $(\Omega, \F, P)$ con $X$ v.a. de referencia tal que $\mu \defeq E(X) < \infty \we \sigma^2 \defeq V(X) < \infty$.
	\[\implies Z_n \defeq \frac{1}{n} \sum_{j=1}^{n} X_j \xlongrightarrow{P} \mu\]
	\begin{dem}
		Dado $\varepsilon > 0$, por la desigualdad de Chebyshev
		\[P\left(\abs{Z_n - \mu} > \varepsilon \right) \leq \frac{V(Z_n)}{\varepsilon^2} = \frac{\sigma^2}{n\varepsilon^2} \xrightarrow{n\to\infty} 0\]
	\end{dem}
\end{teo}

\begin{obs}
	\begin{enumerate}
		\item Hay una ley fuerte de los grandes números.
		\item Hipótesis: No hace falta que sean independientes, basta con que sean incorreladas. Tampoco hace falta que sean idénticas, basta con que tengan la misma media y varianza.
		\item Se podría incluso hacer una variante del teorema para variables aleatorias que no tengan la misma media y varianza.
		\item Existe la posibilidad de adaptar el teorema para que no haga falta que sean incorreladas, solo que tengan una correlación ``pequeña''.
	\end{enumerate}
\end{obs}

\begin{teo}
	Sean $\left(X_n\right)_{n\in \N}$ v.a. en $(\Omega, \F, P)$ con $E(X_i) = \mu_i$ y $V(X_i) = \sigma_i^2$. Suponemos $\ds V(Z_n) = \frac{1}{n^2}\sum_{i=1}^{\infty} \sigma_i^2 \to 0 \implies Z_n \xlongrightarrow{P} \frac{1}{n} \sum_{i=1}^{n} \mu_i$
	\begin{dem} Por la desigualdad de Chebyshev
		\[P\left(\abs{Z_n - \frac{1}{n} \sum_{i=1}^{n} \mu_i} > \varepsilon \right) \leq \frac{V\left(Z_n\right)}{\varepsilon^2} = \frac{1}{n^2\varepsilon^2}\sum_{i=1}^{\infty} \sigma_i^2 \xrightarrow{n\to\infty} 0\]
	\end{dem}
\end{teo}

\allbold{Usos en estadística:} Sean $X_1, \ldots, X_n$ v.a. i.i.d. con $X\sim \ber{p}$ de referencia. Queremos estimar $p$ (desconocido), ¿cuánto de grande tiene que ser $n$ para estar razonablemente seguros que el estimador $\ds \Bar{X}_n \defeq \frac{1}{n}\sum_{i=1}^{n} X_i$ está cerca de $p$?
\[P\left(\abs{\Bar{N}_n - p} > \varepsilon\right) \leq \frac{p(1-p)}{n\varepsilon^2} \leq \frac{1}{4n\varepsilon^2} \leq \delta \implies n \geq \frac{1}{4\varepsilon^2\delta}\]

\subsection{Cálculo de la distribución de la suma y el promedio}

Sean $\left(X_n\right)_{n\in \N}$ v.a. i.i.d. en $(\Omega, \F, P)$ con $X$ v.a. de referencia.
\begin{enumerate}
	\item \allbold{$X\sim \ber{p}$} $\implies S_n \sim \bin{n,p}$ y $Z_n \in \left\{0, \frac{1}{n}, \frac{2}{n}, \dots, 1\right\}$ con las mismas probabilidades que una $\bin{n,p}$.
	\item \allbold{$X\sim \poisson{\lambda}$} $\implies S_n \sim \poisson{n\lambda}$
	\item \allbold{$X\sim \geom{p}$} $\implies S_n \sim \operatorname{binomial negativa}(n,p)$.
	\item \allbold{$X\sim \normal{\mu, \sigma^2}$} $\implies S_n \sim \normal{n\mu, n\sigma^2}$.
	\item \allbold{$X\sim \EXP{\lambda}$} $\implies S_n \sim \operatorname{Gamma}(n,\lambda)$.
\end{enumerate}

\allbold{Técnicas generales} (Funciones generatrices)

\fecha{24/04/2024}

\subsection{Convergencia en distribución}

\begin{defn}[Convergencia en distribución]
	Sean $\left(X_n\right)_{n\in \N}$ v.a. en $(\Omega, \F, P)$ y $X$ v.a. en $(\Omega, \F, P)$, $\left(X_n\right)_{n\in \N}$ {converge en distribución} a $X$ $\left(X_n \xlongrightarrow{d} X\right)$
	\[\iff \forall t \in \R^{(*)} : P\left(X_n \leq t\right) \xrightarrow{n\to\infty} F_X(t) \defeq P(X \leq t)\]
	\hspace*{\fill} $(*)$ $F_X$ debe ser continua en $t$.
\end{defn}

Vamos a tipificar $\ds S_n\defeq \sum_{i=1}^{n} X_i$ y $\ds Z_n \defeq \frac{1}{n}\sum_{i=1}^{n} X_i$ con $\left(X_n\right)_{n\in \N}$ i.i.d. con $X$ de referencia.
\[W_n \defeq \frac{S_n - n\mu}{\sigma \sqrt{n}} \quad \we \quad V \defeq \frac{Z_n - \mu}{\sigma/\sqrt{n}}\]

\begin{teo}[del límite central]
	Sean $\left(X_n\right)_{n\in \N}$ v.a. i.i.d. en $(\Omega, \F, P)$ con $X$ de referencia.
	\[\implies \forall t \in \R : P\left(\frac{S_n - n\mu}{\sigma \sqrt{n}} \leq t\right) \xrightarrow{n\to\infty} \Phi(t) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{t} e^{-\frac{x^2}{2}}dx\]
	Es decir, este teorema nos dice que $\ds \frac{S_n - n\mu}{\sigma \sqrt{n}} \xlongrightarrow{d} U$ donde $U\sim\normal{0,1}$
	\begin{dem}

	\end{dem}
\end{teo}

\begin{ejem}
	Sea $X\sim \ber{p}$ la v.a. de referencia de $\left(X_n\right)_{n\in \N}$ i.i.d., entonces $S_n \sim \bin{n,p}$ y $Z_n \in \left\{0, \frac{1}{n}, \frac{2}{n}, \dots, 1\right\}$ con las mismas probabilidades que una $\bin{n,p}$.

	Si $p=\sfrac{1}{2}$ y $n=1000$ (lanzamos una moneda regular 1000 veces) y nos piden:
	\[P(480 \leq S_1000 \leq 530) = \sum_{j=480}^{530} \binom{1000}{j} \left(\frac{1}{2}\right)^{1000} \approx 87.578\%\]
	Pero podemos aproximar la respuesta usando la normal por el teorema del límite central:
	\[\begin{aligned}
			P(480 \leq S_1000 \leq 530) & = P\left(\frac{480-500}{\sqrt{250}} \leq \frac{S_{1000}-500}{\sqrt{250}} \leq \frac{530-500}{\sqrt{250}}\right) \\
			                            & \approx \Phi\left(\frac{30}{\sqrt{250}}\right) - \Phi\left(\frac{-20}{\sqrt{250}}\right) \approx 86.816\%
		\end{aligned}\]
\end{ejem}

\begin{ejem}
	Lanzamos un dado 10000 veces y nos piden $P(3400 \leq S \leq 3500)$.

	Tenemos $X\sim \unif{1,6} \implies E(X) = \frac{7}{2} \we V(X) = \frac{35}{12}$ y $S = \sum_{i=1}^{1000} X_i \implies E(S) = 3500 \we V(S) = 1000\frac{35}{12}$.
	\[\begin{aligned}
			P(3400 \leq S \leq 3500) & = P\left(\frac{-100}{\sqrt{1000 \cdot \sfrac{35}{12}}} \leq \frac{S-3500}{\sqrt{1000\cdot \sfrac{35}{12}}} \leq 0\right) \\
			                         & \approx \Phi(0) - \Phi\left(\frac{-100}{\sqrt{1000\cdot \sfrac{35}{12}}}\right) \approx 46.79\%
		\end{aligned}\]
\end{ejem}

\begin{ejem}
	Sea $X\sim \ber{p}$ de referencia con $p$ desconocido de $\left(X_n\right)_{n\in\N}$ i.i.d. y $\Bar{X}_n$ el promedio de las $X_i$. Fijamos $\alpha \in \R$ ``pequeño'' y definimos $\ds z_{\alpha/2} \defeq \Phi^{-1}\left(1-\frac{\alpha}{2}\right)$.
	\[\begin{aligned}
			\implies P\left(-z_{\alpha/2} \leq \frac{\Bar{X}_n - p}{\sqrt{\sfrac{p(1-p)}{n}}} \leq z_{\alpha/2}\right) & \approx 1-\alpha                                                                                                                          \\
			                                                                                                           & = P\left(-\frac{-z_{\alpha/2}\sqrt{p(1-p)}}{\sqrt{n}} \leq \Bar{X}_n - p \leq \frac{z_{\alpha/2}\sqrt{p(1-p)}}{\sqrt{n}}\right)           \\
			                                                                                                           & = P\left(\Bar{X}_n - \frac{z_{\alpha/2}\sqrt{p(1-p)}}{\sqrt{n}} \leq p \leq \Bar{X}_n + \frac{z_{\alpha/2}\sqrt{p(1-p)}}{\sqrt{n}}\right)
		\end{aligned}\]
	Tenemos confianza $1-\alpha$ de que $p$ está en el intervalo $\left(\Bar{x}_n - \frac{z_{\alpha/2}\sqrt{p(1-p)}}{\sqrt{n}}, \Bar{x}_n + \frac{z_{\alpha/2}\sqrt{p(1-p)}}{\sqrt{n}}\right)$ donde $\Bar{x}_n$ es el valor observado de $\Bar{X}_n$. Sin embargo, este intervalo depende de $p$, pero podemos acotarlo por $\ds \left(\Bar{x}_n - \frac{z_{\alpha/2}}{2\sqrt{n}}, \Bar{x}_n + \frac{z_{\alpha/2}}{2\sqrt{n}}\right)$.
\end{ejem}