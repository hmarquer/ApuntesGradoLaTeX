\section{Ejercicios}
\subsection{Hoja 1}
\subsection{Hoja 2}

\textbf{7.}
\textbf{b} $\ds X\sim\geom{p} \iff P(X>n+m|X>m) = P(X>n)$ \\
\textbf{Solución:} Lo que nos está diciendo la caracterización es que una distribución geométrica no tiene memoria, la probabilidad de no tener éxito en los próximos $n$ intentos no depende de los intentos anteriores.
\begin{dem}
	$(\implies)$ Suponemos que $X\sim\geom{p}$
	\[\implies P(X>n+m|X>m)=\frac{P(X>n+m \we X>m)}{P(X>m)}=\frac{P(X>n+m)}{P(X>m)}\]
	Como $P(X>m)=(1-p)^m$ (por eso se llama geométrica), obtenemos
	\[\lbox{P(X>n+m|X>m)}=\frac{(1-p)^{n+m}}{(1-p)^m}=(1-p)^n=\rbox{P(X>n)}\]
	$(\impliedby)$ Suponemos que $P(X>n+m|X>m) = P(X>n)$
	\[\implies \frac{P(X>n+m \we X>m)}{P(X>m)} = \frac{P(X>n)}{P(X>m)} \]
	\[\implies P(X>n+m \we X>m) = P(X>n)\cdot P(X>m)\]
\end{dem}

\textbf{12.} Sea $X$ una v.a.d, $\ds X\sim\operatorname{BINNEG}(n,p) \iff P(X=k)=\binom{k-1}{n-1}p^n(1-p)^{k-n}$.\\
Esto significa que $X$ es la suma de $n$ v.a.d. independientes, con distribución $\geom{p}$.\\
Comprobemos que $\ds \sum_{k=n}^{\infty}\binom{k-1}{n-1}p^n(1-p)^{k-n} = 1$
\[\sum_{k=n}^{\infty}\binom{k-1}{n-1}p^n(1-p)^{k-n} = \sum_{l=0}^{\infty} \binom{l+n-1}{n-1}p^n(1-p)^l = p^n\sum_{l=0}^{\infty} \binom{l+n-1}{n-1}(1-p)^l\]
Como sabemos que $\ds \frac{1}{(1-x)^{m+1}} = \sum_{l=0}^{\infty} \binom{l+m}{m}x^l$, podemos tomar $x=1-p$ y $m=n-1$:
\[\implies \sum_{k=n}^{\infty} P(X=k) = \frac{p^n}{(1-(1-p))^{n-1+1}} = \frac{p^n}{p^n} = \rbox{1}\]

\textbf{20.} Cada día compramos $1$ cromo de $n$ totales que hay, con reposición. ¿Cuántos días esperamos hasta tener todos los cromos?\\
\textbf{Solución:} Sea $T$ una v.a.d. igual a la cantidad de días hasta que terminamos la colección, queremos calcular $E(T)$. Se puede utilizar el modelo de distribución geométrica. \\
Si definimos $T_i$ como la cantidad de días que esperamos hasta tener el cromo $i$-ésimo nuevo sabiendo que tienes los $i-1$ anteriores, entonces:
\[\implies T_1 = 1 \we T_2 \sim\geom{\frac{n-1}{n}} \we T_3 \sim\geom{\frac{n-2}{n}}\we \cdots\]
\[\implies \forall i \in \N_n : T_i\sim\geom{1-\frac{i-1}{n}} \implies E(T_i)=\frac{n}{n-i}\]
Además, $\ds T = T_1 + T_2 + \ldots + T_n$. Por linealidad de la esperanza:
\[E(T) = \sum_{i=1}^{n} E(T_i) = \sum_{i=0}^{n-1} \frac{n}{n-i} = n\sum_{i=0}^{n-1} \frac{1}{n-i} = n\sum_{k=1}^{n} \frac{1}{k} = nH_{n} \sim \ln{n}-\gamma + O\left(\frac{1}{n}\right)\]
\[\implies E(T)=nH_n \approx n\ln{n}\]

\subsection{Hoja 3}

\textbf{8.} Sea $X\sim\normal{0,1}$. Definimos $Y\defeq e^X$. $\ds\phi(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}$ es la función de densidad de $X$. Queremos calcular $E(Y)$ y $V(Y)$.
\[\begin{aligned}
	\implies E(Y) = E\left(e^X\right) &= \int_{-\infty}^{\infty} e^x\phi(x)dx = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{x-\frac{x^2}{2}}dx = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty} e^{-\frac{x^2-2x+1}{2}+1}dx \\
	&= e^{\frac{1}{2}} \cancelto{1}{\left(\frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-\frac{(x-1)^2}{2}}dx \right)} = e^{\frac{1}{2}}
\end{aligned}\]
\[\implies V(Y) = E(Y^2) - E(Y)^2 = E(e^{2X}) - e = e^2 - e = e(e-1)\]
Si $X\sim\normal{\mu, \sigma^2}$ en su lugar y $Z\sim\normal{0,1}$
\[\implies E(e^X) = E\left(e^{\mu+\sigma Z} \right) = e^{\mu}E\left(e^{\sigma Z}\right) = e^{\mu+\frac{\sigma^2}{2}}\]
\[\implies V\left(e^X\right) = E\left(e^{2X}\right) - E\left(e^X\right)^2 = e^{2\mu + 2\sigma^2} - e^{2\mu + \sigma^2} = e^{2\mu + \sigma^2}\left(e^{\sigma^2} - 1\right)\]

\textbf{11.} Sea $X$ una v.a. con función de distribución $F_X$ no decreciente con inversa. Definimos $Y \defeq F_X(X)$. Queremos ver que $Y\sim\unif{[0,1]}$.
\[\forall y \in (0, 1) : P(Y \leq y) = P(F_X(X) \leq y) = P(X \leq F_X^{-1} (y)) = F_X(F^{-1} (y)) = y\]

\textbf{12.} Sea $F$ una función de distribución y $U\sim\unif{[0, 1]}$. Definimos $X \defeq F^{-1}(U)$ y queremos ver que $X\sim F$.
\[P(X \leq x) = P(F^{-1}(U) \leq x) = P(U \leq F(x)) = F(X)\] 
Lo que nos dice este resultado es que cualquier variable aleatoria es una transformación de una variable aleatoria uniforme (Método de inversión).

\textbf{13.} Sea $X\sim\normal{0,1}$.
\[\implies \tex{a) }\Phi(1.25) \we \tex{b) }1-\Phi(-0.4) = \Phi(0.4) \we \tex{c) } 2\Phi(1.35) - 1\]

\textbf{14.} Sea $X\sim\normal{\mu, \sigma^2}$ con $\mu = 100 \we \sigma = 15$. Si definimos $Z\sim\normal{0,1}$:
\[P(X > 120) = P\left(\mu + \sigma Z\right) = P\left(Z > \frac{120-100}{15}\right) = P\left(Z > \frac{4}{3}\right) = 1 - \Phi\left(\frac{4}{3}\right)\]

\textbf{15.} Sea $X\sim\normal{0,1}$. Queremos $a$ tal que $P(\abs{X} > a) = 0.95$.
\[P(\abs{X} > a) = 2P(X > a) = 2\Phi(a) -1 = 0.95 \iff \Phi(a) = 0.975 \iff a = \Phi^{-1}(0.975)\]
Si $X\sim\normal{\mu, \sigma^2}$ en su lugar:
\[\begin{aligned}
	P(\abs{X} > a) &= P(-a<X<a) = P(-a < \mu + \sigma Z < a) \\
	&= P\left(-\frac{a+\mu}{\sigma} < Z < \frac{a-\mu}{\sigma}\right) = \Phi\left(\frac{a-\mu}{\sigma}\right) - \Phi\left(-\frac{a+\mu}{\sigma}\right) = 0.95
\end{aligned}\]

\textbf{19.} Sea $T\sim\operatorname{lognormal}(\mu, \sigma^2)$ una variable aleatoria que mide la longitud de una conferencia (en minutos).
\[\begin{cases}
	0.60 = P(T > 40) = P(e^{\mu + \sigma Z} > 40) = P\left(Z > \frac{\ln{40}-\mu}{\sigma}\right) = 1 - \Phi\left(\frac{\ln{40}-\mu}{\sigma}\right) \\
	0.55 = P(T > 50) = P(e^{\mu + \sigma Z} > 50) = P\left(Z > \frac{\ln{50}-\mu}{\sigma}\right) = 1 - \Phi\left(\frac{\ln{50}-\mu}{\sigma}\right)
\end{cases}\] 
\[\implies \begin{cases}
	\frac{\ln{40}-\mu}{\sigma} = \Phi^{-1}(0.40) \implies \mu = \ln{40} - \Phi^{-1}(0.40)\sigma \\
	\frac{\ln{50}-\mu}{\sigma} = \Phi^{-1}(0.45) \implies \mu = \ln{50} - \Phi^{-1}(0.45)\sigma
\end{cases}\]
\[\implies \sigma = \frac{\ln{50}-\ln{40}}{\Phi^{-1}(0.45)-\Phi^{-1}(0.40)}\]